{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nexar Challenge 2\n",
    "Re-Train it with nexar Dataset\n",
    "<img src=\"nb_images/logo-nexar.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import misc\n",
    "import argparse\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import os, sys\n",
    "import shutil\n",
    "import fnmatch\n",
    "import math\n",
    "import random, shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Lambda, Conv2D\n",
    "from keras.models import load_model, Model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxes\n",
    "from retrain_yolo import process_data,process_data_pil,get_classes,get_anchors,get_detector_mask,create_model,train,draw\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\" # importing iPython output fucntioanlity \n",
    "\n",
    "#sys.path.append(os.getcwd()+'/yad2k/models' )\n",
    "#sys.path.append(os.getcwd()+'/yad2k/utils' )\n",
    "\n",
    "from yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body, yolo_eval\n",
    "# from yad2k.utils.draw_boxes import draw_boxes\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitons and paths\n",
    "\n",
    "# path to images and boxes\n",
    "images_dir = os.getcwd()+'/../data/Training/nexet_2017_'\n",
    "boxes_dir = os.getcwd()+'/../data/Training/train_boxes.csv'\n",
    "\n",
    "training_chunks_path = os.getcwd()+'/../data/Training/training_chunks/'\n",
    "training_data_chunks_size = 8*30 # Number of samples of  chuncked downs training data files. Need to be multiple of smallest mini-batch to be used\n",
    "training_chunks_name = 'training_chunk_'\n",
    "images_test_dir = os.getcwd()+'/../data/Test/nexet_2017_test'\n",
    "\n",
    "# anchors_path to anchors file, defaults to yolo_anchors.txt\n",
    "anchors_path = \"model_data/yolo_anchors.txt\"\n",
    "\n",
    "image_shape = (720., 1280.)\n",
    "\n",
    "# Default anchor boxes\n",
    "YOLO_ANCHORS = np.array(\n",
    "    ((0.57273, 0.677385), (1.87446, 2.06253), (3.33843, 5.47434),\n",
    "     (7.88282, 3.52778), (9.77052, 9.16828)))\n",
    "\n",
    "# path to classes file, defaults to pascal_classes.txt\n",
    "# Nexar classes ->\n",
    "class_idx= {\"car\":1,\"bus\":2,\"pickup_truck\":3,\"truck\":4,\"van\":5}\n",
    "classes_path = \"model_data/nexar_classes.txt\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Preparing the data\n",
    "\n",
    "\n",
    "To perform training we need:\n",
    "* class_names,     anchors,      image_data,      boxes,     detectors_mask,      matching_true_boxes\n",
    "\n",
    "Put the training data into a format: Data['class_names'], etc\n",
    "\n",
    "We will load the original data, process it and save it into several npz (Data-chuncks) to perform mini-batches training.\n",
    "\n",
    "Normal batches sizes are 8 and 32 samples, so we will make sure our data npz (Data-chuncks) are multiple of 8: 160 samples per chunk\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anchors and classes names\n",
    "class_names = get_classes(classes_path)\n",
    "anchors = get_anchors(anchors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Dataset folder 1\n",
      "Processing Dataset folder 2\n",
      "Processing Dataset folder 3\n"
     ]
    }
   ],
   "source": [
    "dts_images_names = []\n",
    "for dts in range(1,4):\n",
    "    print(\"Processing Dataset folder\", dts)\n",
    "    dts_files = os.listdir(images_dir+str(dts)) # files in dataset dts\n",
    "    for i,image_sample in enumerate(dts_files): #for all images in the dataset folder\n",
    "        dts_files[i] = images_dir+str(dts)+\"/\"+image_sample\n",
    "    random.shuffle(dts_files) # Shuffle the order of data-chunks: Try to avoid biases\n",
    "    dts_images_names.extend(dts_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     32
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chunk number  1\n",
      "     Chunked boxes data dimensions: (240, 11, 5)\n",
      "     Chunked image data dimensions: (240, 416, 416, 3)\n",
      "     detectors_mask shape      (240, 13, 13, 5, 1)\n",
      "     matching_true_boxes shape (240, 13, 13, 5, 5)\n",
      " Chunk number  2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4984fc5a33bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m### Preprocess the data: get images and boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# get images and boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'     Chunked boxes data dimensions:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mimage_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dtorres1/Desktop/Nexar_challenge_2/src/retrain_yolo.py\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(images, boxes)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Image preprocessing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dtorres1/Desktop/Nexar_challenge_2/src/retrain_yolo.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Image preprocessing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Custom dataset object \n",
    "images_list = []\n",
    "boxes_list = []\n",
    "\n",
    "# Load the train boxes\n",
    "Img_db = pd.read_csv(boxes_dir, header = 0)\n",
    "\n",
    "samples_to_chunk = 0\n",
    "chunk_count = 1\n",
    "\n",
    "random.shuffle(dts_images_names) # Shuffle the order of data-chunks: Try to avoid biases\n",
    "\n",
    "for image_sample in dts_images_names: #for all images in the dataset folder\n",
    "#     print(image_sample.split(\"/\")[-1])\n",
    "\n",
    "    ## Get original Images and Boxes\n",
    "    # Get the image\n",
    "    img2 = mpimg.imread(image_sample)\n",
    "    images_list.append(img2)\n",
    "    \n",
    "    # Write the labels and boxes\n",
    "    labels_boxes = []\n",
    "#     print(Img_db[Img_db['image_filename']==image_sample.split(\"/\")[-1]].as_matrix())\n",
    "    for box_matched in Img_db[Img_db['image_filename']==image_sample.split(\"/\")[-1]].as_matrix():\n",
    "        labels_boxes.append( [class_idx[box_matched[-2]]-1, *box_matched[2:6]] )\n",
    "    boxes_list.append(np.asarray(labels_boxes))\n",
    "\n",
    "    samples_to_chunk +=1\n",
    "\n",
    "    ## Translate to training model's inputs\n",
    "    if(samples_to_chunk >= training_data_chunks_size or image_sample == dts_images_names[-1]):\n",
    "        print(' Chunk number ',str(chunk_count))\n",
    "        ### Preprocess the data: get images and boxes\n",
    "        # get images and boxes\n",
    "        image_data, boxes = process_data(images_list, boxes_list)\n",
    "        print('     Chunked boxes data dimensions:', boxes.shape)\n",
    "        image_data[0,:,:]\n",
    "        print('     Chunked image data dimensions:',image_data.shape)\n",
    "        ### Precompute detectors_mask and matching_true_boxes for training\n",
    "        # Precompute detectors_mask and matching_true_boxes for training\n",
    "        detectors_mask = [0 for i in range(len(boxes))]\n",
    "        matching_true_boxes = [0 for i in range(len(boxes))]\n",
    "        for i, box in enumerate(boxes):\n",
    "            detectors_mask[i], matching_true_boxes[i] = preprocess_true_boxes(box, anchors, [416, 416])\n",
    "\n",
    "        detectors_mask = np.array(detectors_mask)\n",
    "        matching_true_boxes = np.array(matching_true_boxes)\n",
    "\n",
    "        print(\"     detectors_mask shape     \",detectors_mask.shape)\n",
    "        print(\"     matching_true_boxes shape\",matching_true_boxes.shape)\n",
    "\n",
    "        # Save\n",
    "#         np.savez(training_chunks_path+training_chunks_name+str(chunk_count),\n",
    "#                  class_names = class_names,\n",
    "#                  anchors = anchors,\n",
    "#                  image_data = image_data,\n",
    "#                  boxes = boxes,\n",
    "#                  detectors_mask = detectors_mask,\n",
    "#                  matching_true_boxes = matching_true_boxes\n",
    "#                 )\n",
    "        chunk_count += 1\n",
    "        # Remove data from RAM and update: Our problem is low RAM\n",
    "        images_list = []\n",
    "        boxes_list = []\n",
    "        image_data = None\n",
    "        boxes = None\n",
    "\n",
    "        samples_to_chunk = 0\n",
    "        # control\n",
    "#             input(\"Press Enter to continue...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 B - Data Generator\n",
    "\n",
    "Get the datast location and create a callback that generates model inputs on demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.05\n",
    "random.shuffle(dts_images_names) # Shuffle the order of data-chunks: Try to avoid biases\n",
    "training_set_files = dts_images_names[0:math.ceil(len(dts_images_names)*(1-validation_split))]\n",
    "val_set_files = dts_images_names[math.floor(len(dts_images_names)*(1-validation_split)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(list_filenames, batch_size):\n",
    "    Img_db = pd.read_csv(boxes_dir, header = 0)\n",
    "    while True:\n",
    "        for batches in range(len(list_filenames) // batch_size):\n",
    "            images_list = []\n",
    "            boxes_list = []\n",
    "            image_data = None\n",
    "            boxes = None\n",
    "            for image_sample in list_filenames[batches*batch_size:min(len(list_filenames),(batches+1)*batch_size)]:\n",
    "            \n",
    "#                 images_list.append( mpimg.imread(image_sample)  )\n",
    "                images_list.append( Image.open( image_sample )  )\n",
    "\n",
    "                # Write the labels and boxes\n",
    "                labels_boxes = []\n",
    "                #     print(Img_db[Img_db['image_filename']==image_sample.split(\"/\")[-1]].as_matrix())\n",
    "                for box_matched in Img_db[Img_db['image_filename']==image_sample.split(\"/\")[-1]].as_matrix():\n",
    "                    labels_boxes.append( [class_idx[box_matched[-2]]-1, *box_matched[2:6]] )\n",
    "                boxes_list.append(np.asarray(labels_boxes))\n",
    "#             print(images_list[-1])\n",
    "            ### Preprocess the data: get images and boxes\n",
    "            # get images and boxes\n",
    "            image_data, boxes = process_data_pil(images_list, boxes_list)\n",
    "        \n",
    "            ### Precompute detectors_mask and matching_true_boxes for training\n",
    "            # Precompute detectors_mask and matching_true_boxes for training\n",
    "            detectors_mask = [0 for i in range(len(boxes))]\n",
    "            matching_true_boxes = [0 for i in range(len(boxes))]\n",
    "            for i, box in enumerate(boxes):\n",
    "                detectors_mask[i], matching_true_boxes[i] = preprocess_true_boxes(box, anchors, [416, 416])\n",
    "\n",
    "            detectors_mask = np.array(detectors_mask)\n",
    "            matching_true_boxes = np.array(matching_true_boxes)\n",
    "            \n",
    "            # yield x_batch, y_batch\n",
    "            yield ( [image_data, boxes, detectors_mask, matching_true_boxes], np.zeros(len(image_data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x720 at 0x7F2812110B00>\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x720 at 0x7F2812110B00>\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x720 at 0x7F2812110AC8>\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x720 at 0x7F2812110A90>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-795e677e4b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#     print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-bdcc6fece533>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(list_filenames, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m### Preprocess the data: get images and boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# get images and boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m### Precompute detectors_mask and matching_true_boxes for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dtorres1/Desktop/Nexar_challenge_2/src/retrain_yolo.py\u001b[0m in \u001b[0;36mprocess_data_pil\u001b[0;34m(images, boxes)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# Image preprocessing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dtorres1/Desktop/Nexar_challenge_2/src/retrain_yolo.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# Image preprocessing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in get_batch(training_set_files, 32):\n",
    "#     print(i)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 A - Training with Data Chunks\n",
    "\n",
    "Train with the data processed and saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will iterate over all the chunked data to create true epochs and will use mini-batch training\n",
    "\n",
    "We will use 2 different training steeps to steer the nerwork to our purpose,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# Wrap the Yolo model with other model for training: You can select how to wrpait and if you wnat to do transfer learning\n",
    "# Create model around yolo model \n",
    "#     Use freeze_body for doing transfer learning on 1st training stage \n",
    "def create_model(anchors, class_names, load_pretrained=True, freeze_body=True):\n",
    "    '''\n",
    "    returns the body of the model and the model\n",
    "\n",
    "    # Params:\n",
    "\n",
    "    load_pretrained: whether or not to load the pretrained model or initialize all weights\n",
    "\n",
    "    freeze_body: whether or not to freeze all weights except for the last layer's\n",
    "\n",
    "    # Returns:\n",
    "\n",
    "    model_body: YOLOv2 with new output layer\n",
    "\n",
    "    model: YOLOv2 with custom loss Lambda layer\n",
    "\n",
    "    '''\n",
    "\n",
    "    detectors_mask_shape = (13, 13, 5, 1)\n",
    "    matching_boxes_shape = (13, 13, 5, 5)\n",
    "\n",
    "    # Create model input layers.\n",
    "    image_input = Input(shape=(416, 416, 3))\n",
    "    boxes_input = Input(shape=(None, 5))\n",
    "    detectors_mask_input = Input(shape=detectors_mask_shape)\n",
    "    matching_boxes_input = Input(shape=matching_boxes_shape)\n",
    "\n",
    "    # Create model body.\n",
    "    yolo_model = yolo_body(image_input, len(anchors), len(class_names))\n",
    "    topless_yolo = Model(yolo_model.input, yolo_model.layers[-2].output)\n",
    "\n",
    "    if load_pretrained:\n",
    "        # Save topless yolo:\n",
    "        topless_yolo_path = os.path.join('model_data', 'yolo_topless.h5')\n",
    "        if not os.path.exists(topless_yolo_path):\n",
    "            print(\"CREATING TOPLESS WEIGHTS FILE\")\n",
    "            yolo_path = os.path.join('model_data', 'yolo.h5')\n",
    "            model_body = load_model(yolo_path)\n",
    "            model_body = Model(model_body.inputs, model_body.layers[-2].output)\n",
    "            model_body.save_weights(topless_yolo_path)\n",
    "        topless_yolo.load_weights(topless_yolo_path)\n",
    "\n",
    "    if freeze_body:\n",
    "        for layer in topless_yolo.layers:\n",
    "            layer.trainable = False\n",
    "    final_layer = Conv2D(len(anchors)*(5+len(class_names)), (1, 1), activation='linear')(topless_yolo.output)\n",
    "\n",
    "    model_body = Model(image_input, final_layer)\n",
    "\n",
    "    # Place model loss on CPU to reduce GPU memory usage.\n",
    "    with tf.device('/cpu:0'):\n",
    "        # TODO: Replace Lambda with custom Keras layer for loss.\n",
    "        model_loss = Lambda(\n",
    "            yolo_loss,\n",
    "            output_shape=(1, ),\n",
    "            name='yolo_loss',\n",
    "            arguments={'anchors': anchors,\n",
    "                       'num_classes': len(class_names)})([\n",
    "                           model_body.output, boxes_input,\n",
    "                           detectors_mask_input, matching_boxes_input\n",
    "                       ])\n",
    "\n",
    "    model = Model(\n",
    "        [model_body.input, boxes_input, detectors_mask_input,\n",
    "         matching_boxes_input], model_loss)\n",
    "\n",
    "    return model_body, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     I found  3200  data samples on the data chunk\n"
     ]
    }
   ],
   "source": [
    "chunks_files_names = fnmatch.filter(os.listdir( training_chunks_path  ), training_chunks_name+\"*\")\n",
    "# Load just the first chunk \n",
    "data = np.load(training_chunks_path+'/'+chunks_files_names[2])\n",
    "print(\"     I found \",data['image_data'].shape[0],\" data samples on the data chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58277778 0.51976285 0.06333333 0.07509881 0.        ]\n",
      " [0.49555556 0.50395257 0.02       0.02371542 0.        ]\n",
      " [0.46555556 0.51778656 0.03555556 0.0513834  0.        ]\n",
      " [0.43111111 0.50395257 0.02       0.02766798 0.        ]\n",
      " [0.39722222 0.51383399 0.03666667 0.03952569 0.        ]\n",
      " [0.28111111 0.51778656 0.08222222 0.11462451 0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsvWmQZMt1HvadzHurunu6Z95sbwMeVgKQAIECQBCgRMhcYRBEUJBCIQVFGaLoBbYseFEoaEN2WJYdCgUlhRZTNhUBhimRQYkwHJIMkgZJQTRNUAIpgABJ7Pv61nl4s3X3dFfVzTz+kcs9mTfz1q2eGbDhmDNR01X35r6c852TJzOJmXGP7tE9ukeB1O93Ae7RPbpHp4vuMYV7dI/uUUL3mMI9ukf3KKF7TOEe3aN7lNA9pnCP7tE9SugeU7hH9+geJXTXmAIRfR8RfZqIPkdEb79b+dyje3SP7izR3fBTICIN4DMAXg/gUQAfBPBnmfkTdzyze3SP7tEdpbuFFF4D4HPM/AVmXgJ4J4A336W87tE9ukd3kJq7lO6zAHxV/H4UwGtrgYnonlvlCYkA1Bpv7F0xPBFKyLGWzqbp//6TLPE3SunvaDm/xsyX1wW6W0xhLRHRWwG8NfxW0P455+EAAMycfAcAJj+QjYVSqjigJxYGjD59mUdfvjRvIoL1cZg5vo/xKS1zkh2n7/J6Jc9oWBb5XjZXbBefNhGBh9nH+Hn45G+hKUNaRARYTtLJ61qqC4CkzcK7kJdLpw9LWiVpuXgmyUMpF8ZaO2jrPh8CM4p9Ecvl4ydhrGgE5Z8zgNI4U+W0S+NWtrUMJ8udPycCmK37zsM0XLhx4G9hvjwawNPdYgqPAXhE/H62fxaJmd8B4B3A7SOFmoSbHBeVyVuTnCODayrdFhMrkExLaw1r7R1L+04QEbmJp6b1lVIKtsA4ZXrT+4GwLuimfTrGyO8GMfd/Qy6xDHcY8dwtpvBBAC8ioufDMYMfBPBDUyLmE7HEaUtUk8rr8qrFH0hxy4OBOJUZDSQEhuXM07HWo59CWZM4IpqU2iIQIOoSJOIoSnBiKckrMM9SGaa2gUuj9C70b18Ha22CcvI88t+19nH5OrRQi1uLXySP5ktplBhxPkbytg/PBii4OJ6VQwywAGXjCmV0dxK6K4ZGZu4AvA3ArwD4JIB3MfPH18W7m5y2ROsYyRikK3V0baDKTq9O7ox6BrJ52ZP0RlSiJJ8N0NaUSTW1L0vtSEQAIaoGY2237rl7N2T+m461WE4u5znWtnn4kP9YOWTdS+FK6d4puitLkhsXghQrpV0FjQUjtS2k3JUKkk15KWiq8eQzSQkSoeyZTVHKALH4wQukun2v77uHFgAmwOYip1ep7SBIWwZH3b9quxDlzdWVfFDltguFcQlXQlnJc1VHfJJCHdbZVfJy5xTKK8MGe0UimVVabplPsWzhna2EoaEto09XIQ4QYYtg6hGFW72v1ysZE4oS20nNJiHjEfXCyoI/xMyvLmYk6PfN0FiiIBn7yqYwqzcapc/z72PP8ncDdWVCOUsQEGsMWSVj3KR8Qh5i8jMAUl6CFOqQ5xv+SvUhqQcBTu8Wg8xOUwtkOiXGs07Ku4BI4HCpDpHpVIvEiaoU4kmon6svpUlVEwIllW8d1ZjOSaV7iRnn6eboMokzsT9PCVMQ8NVbnJHpy/3ASGO6CvPg+zoqdkwYU2Hi1UobykLoB4vIOuiWyYSoJBYmk8s+lZp5HUNe4TURgUVeNUZXk8IJQwMGKxUlNDBGJYYwVq4+o0KZZDruS7U/SHyMfE40mCD+RbF9SgwhPmcr3qf1KoYXJYv1j+WQYYd1T/tLIBEaN04nzFP1dppNDZGnZu/DOqk/BvHkpxRvzB5QsxtMoX6SrlcNalIjMIS8rCU01HN898fyELoOyphNzLw88vdYO56UZD2mILcSrUMbbuoAtgDjZb6OOfdqjmzvGkMYpre+vH34+lieatcIfdfzs7Jtolbek9ApQQrIoA0PoE7fkZMAvkiyl65TpB0hXSEYZVBeevWWXy/NBipJAnm85IOH8wzl07MeDSSwO0AWTq3/5AwL8VnOdHK7i6SyREIPsjxakmV3+rH7XUJRg3b1bRjjO/2gn5TxWUGFkxLPo8NeurJPJmMAJeDn80uY94g/ygAZhXoLtSQN7xsrU336HEW7Uo8KmJGgzJhXTMK3WSblS+pD3s9x7AaknQgaiaPqdGqQgreeIZjRctpMigcwKZIvMIQy5Avl2IxyqJrknf8QD0rwtgz31+S9pn0kIsnzC2loGSZvwmGTFtPpH4q/jIE6uEkL99J5s3ib0qAeodzDkD3sL7yNz6LKI4UcD8ZAzCvJN0szMpSJ0j+OYddx1k6Mh9PEFDytq3R1IotnJaPeOgi9DjqX8pJxAjQdg7o1qVSa1LmhqyThar9r9ZCwufTO2UKmD55SO+fvQ/1yqRa17pE+RCXMVAFR6v+cdUsIPmibgtpZU8dK8ZP6rhkP6+owBeXGdG9TqJ46pgD0kPkkDVnT3ZP0K4xj43L6zlYCLLJ4l4eTz6y1RcZVqoP7W58YtYFXUiXks9LkU355t5RubSLIcuQ6fEmNGe1D5Q24Wrll0qx71tkXiun7NJmGcDynpH1G0pf1CcueNXtV+pzgLSCTVOFNxygJdSmvz1Q6VUwhh/BTGUAtjVr8KVx+Sl4l4xxXwrOHz7lTyhh6cHGGOmVtkpbUj5z5rTP25eUZQ08nSW+s/DJcLY91jKXEGGuTtdbfNQZbYnC1uq2jdUucJxGIsV1v0+35VDGFIVH8TFWlgID65OCZHnlTrlx+Uf9e2rizJpPUBlFQUWoIQ1Jpco9KfgY4bBKKEZDYW6agspOSZGSbJr9ReXIzQk2QZH+duhhNguIjk6TBu7W0JtjkuoUsWXwm0ulZffAUKm295RRwOyiZjR8oQ4trHpeE6mGMQWih0uQpxx+3W9TexfQFTE1gpnRmQD/wbTb5iohBxJF/w3fpOFSC6zV9V1LiDRhK7I171iWUlK/WnkWmJNInoiICkpSWNaSwnmp9W0o7eoOi7zdAMG7RIEwWCKgjlo3AbquuqKXv+6DyiSKE4nC/dBSjhh2/IZ2+D0WdeIgCqsgp9MUm0tTTKUQKvhJyAInneWcrpRJol+txpQG6DuZNeR/yKNYgM0iVVIFcb8/jj01oSaH+JfvEGKSdYlNJ3znGU1rFyOudMyxmAS9QRxhyebWko8t6pQ+H5c3HwdhYkOHz+Hm4cZUqG6NJGILb0DRsv1of1JjlWD1q5d+ETg1T6CtVMEipsiSqDZx8QgXKoXsNytcGY82GMTAoWe+3YN05C8RIzlso6fkyz5JET+F0/zcYLJ0EA6DqtooS41g3eJjdqgTZ8mSufR+kW0E48ntuIMsZ6xTaZOLLd0SU9Fl+PkYSzkFRL4mt+1gT+z2WV8kPAAXYEfQS+xHojaNCKCjlyhfySfbDlJjYCdW6U8MUAqVQu39WUxnWGZ7WQep1EiSQZCCDwZyFVUol22hLTKr0t8QActQRJ6lUS7LJmEPoMUi9brK5wZiuSNQkfR4vaadCfoOJJmgdw08flt/VGFYtvRLizMtTSx8TGBizRQoUyuFK6lno91LYUpsMkcp0OjU2hX6ghQfWcSxrnAQMKhqvl9o51Cu9zxlEqTx55+TfQ7jSxB2LV5Pk68IV6xvU1iDpsnRKsDd/FgefrJeQUnK3aE45sy2Vn4iiF2dIO6CcQT25MFVU79JW3L3pI8ay9oUbHOoytgo4aGvlcvRJ+TnmGZiX6PmBS0QEK3BG374MwMbxG94FL8NUreiXq4eemjbtZ9dRPt+hOnaSjVynBimMSeswaJRyjb0OFcjfuRQtSeBNpeA6pFCKnzOgdZKrVBZpPynlU6t/LpFLE7cKrUdgf17Gqe1Qk3ql8JNoDVocy2cMHZTC3gkayysPt0meg74+offnqWEKQJ0xJNJIDr6AIArEXuSwDLdhG1l/Jl4iQV2hhvnJTs7zIoAyA9065jAF1pfUlr6c/U65YhnvIA1h9fiwIkjd/K4Uqc+rUuUx5lsL+/WjiY5eOQV7Q3ClPiGdGvUhp7yDJFNwfwUHx1A3BOBd1zNuy2mYQfohGDMoHofWY9qIBLO0E1RDSVauhJTmOaYCjb0vUayv929Plv4KA6SmG1fk5OBNri70TLvO1NMUKTsItdwfsbzyWUl9pGEZY1wu6SPleoxRzV5QekaV54N6CWHHybgsu1PHuAWVzaUl6gx7YqRwW0yBiL4EYB9uG3vHzK8mogsA/g8AzwPwJQB/hpmvrUurBPX7jOrGRmanN0kVIP4tSNMxSDaA1oqKMJepl0BBrVk3gSMjK+j9pXIECmmXICwRxe3Trh0KeXLPFGT7hTMgY7gRyn0gZFpJXoAY3P3z4sShfuUkNyqvo0GYkYl/p6hkkwnPi4xvBPVOpZxhxd9rwvtfIyHH6U6oD9/FzK/g/pintwP4VWZ+EYBf9b/XUt+IQwmU60qhc+JAJPex4Pi3BPGJKC71hLCJX7x/bv2Kea6Tx3R8nglaWdPhRUkoypOXIf72aVoOJfOfMEB5aB9RFRmRT+xBO0qpxP6D9NCY2B6K3BHsyu2XCH2nFJC7MuRnRkCkCWDQH3mb5OXP61SzscS+YSRLeLFNZTuTcp8BA7NgtqKN3TPyDkjMBGsBZooOWnISp0jArUsmen8sl1weJ4QjBlNm5P0clCsrlHL5MpK2699TUsepdDdsCm8G8NP++08D+BObRM6lZC2MXCabShsZbu6y5FknMYYIhcVnnKbUcSpKmJreSanEdE8fDVUM1255263fKzIpN06lvGQWqcEq/9uXt4SQp9LtMgUG8K+I6EPkLncBgAeY+Qn//UkAD6xPhorfpwyWnIkMoFY26ErPx/TZKWWpQeQ8TO7AFP6WyjL4mw2IpN7ZoLDWIm4o42FdJaqRbXa7E7QWd53RbEzay9+1/pI2hxz9TCvn0PuzVJf1E2uz8ZqnV0NtMl4RLQ2yTVWdTfv1dg2Nr2Pmx4jofgDvJaJPJYVlZqrsD6XshqihzSBsgiIwxpevcmNMTYeVeZUMTLneWNKfB4YfHz2eGkyuzOGZrJu0jcRQTk9xvwsnPjP5MBhuiZVhFQjsfeajDSBjPHn55btcpcgNsgPVI2/bERtL0q+VsVlq61J/WAIoPzCE/elVvtgl+J63Q/pbjAcl8iww4LQNKbEvOdtGj8CKbc0aUMI/w4Yy5l4GAJjiyc/VMajI97NCuEEqlG1Y3ml0W0iBmR/zf68A+JdwF8s+RUQPAYD/e6US9x3M/Opgi6jp5jUOl0/mEkIIv/NPKd0aM8nDyE+Rsw/G65DByPhTpVtOUr0IBjtZj9IEkO2Rl7FUx7zupfYoPaulv65+uZQutS9V8p3SfjUpLaPlnqh5WzLLctZV3Dy/mCbKqHLdSkFt3G6S91Q6MVMgojNEtBe+A/j3AXwMwM8D+GEf7IcBvHtaehwNVHnnBV90JTh3whSEISmRxAXGIdMag4oh3bB3ofaRaZUGdYnZ5YM3Qnjv064yCTUG/3IGlQ+ePA2tNbTWSfwYT/jVS4NfrW3iUiIHhyTET0kdUEq5ugnffQUCTJmBxfe2/xA741qNkrZS/X2fJSYX0iZOyxpht+hfmA6wBuSNvAjtys7oGD9swWzqk5IsnFT3S7hKe6OgjmmmpMQnHM6SChYZtooIqy02pNtRHx4A8C99IRoA/4yZf5mIPgjgXUT0HwH4MoA/MzXBtCJy7bZ/OqZbAYiDDuTgdE2qhE73D4Qbbjnd0AlSkpTQRY4KavUM78LS4ICBWaeKsC9fiCcPaQnprFsSLaGSWvllGUtlHkxeogyyp/nKtErtwuxdkaulH6o/rh3KkD6pWxJgJIMpRP3G96AtMaerVDFfGt+VWUU1UVWsk0t3qArVEoljnKY3wSm5IYq4UelNOa7CYYlIQL2MQTg9bZBe2uiU6v4xrazu5BkJfPzcgDehHq7DSZTNA8NSOwcmU5LqcZDTEAGFsgdmsE7tUUrBGJNMrLyN8oFts4MASndpWmuhtAJsgMVAPPV5TFpWaHD0WrRTFFAEpcIiZ3QyvfH9Dj2q8LXq26Kw54MBP8E82imMEZek8C7N+6d0m7UkWx7vSGxrIyCfLajw3sJMuiHqVLk554M0rA/fTjr+Qf8V6/T39QaadVJZhgn5rQsvqXZ+o4Po5ctfamUr6fildOUS5aC8lKpHSRhGPAehpDKUypSTVBfGwoypUXm4dYxyU+rrHP5mXKkQXpYnTWMN1TSPJL+xdETbn6AJThVTGNLmDOF2BoKLWzZGTjWWnYSk8WpaGfP445uM7hYaHE6+aRP260GDfL6OiLjW3pTDmxPQ16P9TtXeh1zKOsmQhWEB4Qq6cVUqMvoLOKh/lx+0EuG+V81yCSX1dyIFax1s1npoq6idRNyfh2CjxY6hYoTEA7Cgo8s6y+cllSB/L6kEuaMBC0LiBkU6y3uIDJy+G+4YkCik1j9JekoBEnUE3Tk8ZuuLERh0HZ0gz6vCTPvhIfTwQnslAgLcq4SFtnPqnisgcSgoAAqu6MKmFG/56pEa+bBDu44vS7R9UVQ/ZFgiAgVHN95c7p9KpCAnYWkgE6VhxiRQzmhkOnm80klMYzBY6rcluJiWue6glIepSfxS3vl3mXaNichnY1Db2UTqbVgKHybqSSB8nleOzgaTr1CuGK+A9gb5VdpKpjWoHzK7RZZP+jsxegzTKqDPWlvLOg/SHlTs9tDhqWAKiSGO0B+qgqFRjIiiBAf3HLumr+XPw28lfrsVHwYUg8kOBlVd4pbPNpATnKi8OiDTdasPqW4umVZt8Mu0xphirW1Kkjy3MWCDiVOaLCVVbKycyT6FIGmB4qa38H0dI8/3qeThpJ9H6X2KJNdTX+ewVNkDlTyXvk7hIh5OQqVM0lWEA1SuFiDME+f8dBr2PmxMgfsC/Rl2+c65gVRluOvS7XCyraNwd6MYWgBZ7zlpwTAx/6kGrhC2VAaJAGrnQrpnNrneqwa3S3E3lQylMoTyhe8nTeeO0QmFXYlh8kg5q3UQgqnGkEvx8+SkYK+XIhgxx9t9LUOopj2dTgVTAIZc2RZ11iE0lb9raeXphF8qTPaAHhQl8FfGya3ILky5bDnJFYPSjkE56OT6e66SDCHkUCLnqEm2izz5Oafg1CTrMvTWK/dHEuI2mUSuIuTP103IfOfqJlRS5+Q7otQnoy4sUljgkOcwr3wcl/KsqUzryMUrO/Kto1NjaIyTAgTDduAjUFIjhmoFg1Wcqal/vl9CYmvdleU+vEtcOU859Lfyuu/DnYQOwZFwKgoNTzFm2Nqs0CRQXA5spZQ3V2X1BAZ7Jmo6bN4msl1KbWvsyr0vrI33y6AAw4Ko6dP150qUhlWed3DGKqGm0iRP2qOmHvFQn8/hcBCg1nrHJtkG/txGF0bFceF+h3A2aWMiguUsHR+LeYj2UqHhBIYFQBTqxf0ISZCDFv3sw7Efr4KXhDR8pGSyD2wfYARnJeev8A2KFALFxs0nPOrGk9hBlclQDItUqpYmmNyIVZ5oBXtH1lk5GqhN2tokYuaiylGyW9QQTj7ZrLWwbAb1C8xNxVWAHtISAfkpa5JhyXKW6i3Lk1NgJlOl2Vi4nIFOVatKfSCzKaEG2d7rt6KHTX6BIzhUmqKDcr3GUEK53H35awhrjE4VUoiDzFonzSrMoMQk4jORXg/JleOezINmzwdq4sQTbrqVFg7uDYJKqejpJ8sjuX/yPIOLENJPtoMS8aQEDRNf3vWQM6WcATDgDkMRrePydr8JKhtYvrxBalG/+46JHWfgYb1knvJ7ruqMwePawA3orK/BONVUGtceoU+HZcjLHYFkNs5KEzFNY7hhLs/L5VGSyWGs+jhKpm1j2ccmuXulwJz3TTVKVoJTRnKg1zh9SfqVvtc4a6Bcv8/TVcp1SpKO+FpiVmVdfEi1M/xDurIdSlJXlrmGpAgEpVVkLskERY8U5N6LeFlJXAaw8XsCYQvlztMaU2XuBuVjZxOSZe3LXGd6tb9jKKZnkoHH2WIb5XHEr9E65GNjHcKu0alBCrWGkXoqIKVN0N0Rr09n9vsVvE7G7mUcJBExJOn0eSvSXh9TbiJYIUm1jXoeIUgdAikd0UPcgMVuMjGE8xX3kzPkyWCoONncbr28+2pSOFLYTERuN1+y9RcMYgaBwdYM0KlDZTkjcfE0WsRjyHr45ZBDYcksL2MJHUwZnDk6iuVcE3egxkHaIvKJJYUIoT7ZBKJgWbde5w+7MFOyCHaFskoQdjwCbJwdi8VYZECcrZHeBxHiSYEQmU0soxxHwUFrOmM4dUghp/UDaf1gkX8lDaW1NPwRUmtWD6mjrh3fcBKsL3M6GIdSTErfIYQtUTLwk2x7qD5EKmUJLymRlAAQDrmRN/Fg2JZ3U/KfhKbaJXoal/A9stqknuPtnTLJ9X1ze7R52qeOKeQ6Z/gr1Qn36cMw6hIpmdYVuB8oHGPmQ2cwuPd4jJOvkJ+cnOm7wp5+BAPh8JLYkgqSlzdBPQIlJPYZ0WYldancNgRrDXLK1YKaraeU5jp4nfd1ns/YZN+ESQUnqJIdIfwt2UU2nVs1tbe3CRXKltW1VOd8XNXKHc/5CFrgBuU/FUxhHW9PJmJ2ngCA2GFFw5dlB3tD41T03ZB27JjBkmgPt2NHYKiGhHRCmN4zMK1HCJ9Y3b3KkjOE8Lvk4yAPgpGHp8gw8nm4OZr8MuxQp03bpNReksnUdPh1DLg08EtMpZYO+brLg2FCO4Rn5XyGDDLkNWyL+n6NWpnT5ykKCHeM5m06hfmV+mIgZEQd5YEsm9KpsSkEqkmfMQMbUWpkkhe8rpMyeR4DaZxJ3SQeeicrWabS5bKOq6T5Ws7X9AGldML988tqQ/r5wSwyXLqhKmVAIV1NvQ9Fbsxy6Q59I0r+FqX2Ky1NSuZRH8woPg/bz1NBwANhsCnlZcl/lxlChiTW54LQ8cX8MGyLWjsN2qAQR4Y9KZ0KpjC1+Dm0dMac3gsxUH4rs4wvVYDYyNFG4DswRhMegBaIOjUoLhspUmKnm++MWJ4+rSIUhPJuzX05DXcygKhr2FFpk+c5aZ0yFeeo5NsM4SCbfmlrsEu0AJXD+9wGU4PhJak3Fm4srdAM8g4M97I0HrL4LlBQsvqbuwqIMgoDTg2rgJ+4Cf6m4VcB0fvoE5kOEcDWGZr9eCD2huswfvz/csdqzpzzdHNmXdRZCrRWfSCinyKiK0T0MfHsAhG9l4g+6/+e98+JiH6ciD5HRB8holdNKsWUghaX+aRhsDwga1Iol47lTyFIyLZPqC7poEQZU/gdYHyjm0xaS2uxkBj+mZuYveTWWicqlUQ60m1ZKQWNBgQFrRpQ3I88LHVYPw9p1ewbxTqvafcpVFQbTiD48h7eNBKzjYxhHW1S2yLSSBBQeTxu2qInRQtTbAr/BMD3Zc9qt0C9EcCL/OetAP7RJoUZG0g1/bJ0nFmYZMmFqyOGJZmeNMytM44l5R4ggrrkk8eolYxqMp1EHfF2g6BihPD5UWuJFJYSldx+DxP3YuhqPWX55JJwTa+tqQQl3Ve2dU09PClTmRIvt72MSdwp6ZUmn0xTtnEJLUwtez4HikhnTXmm0FqmwMzvA3A1e1y7BerNAH6GHf0WgPvIH/c+IZ9qwfMBKOPUHIVy/SrXqZ3aYNyHbXFipCqBAkE7CO6/xyy8bz0xoMlJb7mcKcsvjU254VA6GBFRlPRO9/RXjkEPLOhhkMtnxhgYYxC8ESWzCudhKtLFgSjzDu9NWE8vTJykTQsTrDaQx56VKDmrcmI6tQlShPEbEHPv/zDFWU0yyVpeA8bIGPiE5IKrpKKte7aOTrr6ULsF6lkAvirCPeqfraUx2FljGGNhZeMFyiddTZqVpHZtMOfxwmQMkzyggrAt2VobJ7E1YcL25W6aoZknH3REFBlDrUxx12NhwMo2kVJTbpuWDLREoU4y33yQlt7lEnMTROYDeSZ5cqqhzinhZPjgIFcbm/mYHmNadRV0iN7yMueXEIfvyTjfgOfdtqGRuX4L1BhRdkOUfzYWfjBQSZU2MqWejznXTGB20RvNIxBqUntSkcM7r0oXLpXWDq7bZNcgh3qEm6KJAPKTkipSFJS9I4SzH5xGoAcDy7kvD5FS2AVKCmAbytIB5N4rb/hUSsHYJZTSsMZC0RCJxPIwwIqguM8LZAHWCbPLGUOJiYxNwNgvXH6X19X/cMeV2XyMoHpytPRQ7K08nCxnJyeKY/gsOLD148/7v4SVIBX2VRDI3wBlvaXSTeDeD2fQNv5GKNl+kZmD+zFLGXojYCpjOClSqN0C9RiAR0S4Z/tnA+LshqiSLi3CDqRS3sk1qZ+/L14tX+CyuU5YPHQkZjOUOnKJsDRwASR6emnd3O1mzNeaOTOGlZdMZZ1k3ZxKEYpfN8K68ydtdCGvwV7pQCXzCkUek5jrGEIOl/Myyo+Mk4erpV8imV8ubQNjr6lG+e+kjJX88vI7VXVaGceEaF7vr4f6ULsF6ucB/Hly9G0Abgg1Y5TWoYQp4abkURuIsnMGut2aPOW4cJDSpSGt/5JSo10YMENDXfgEfX4srbRO63R4/5x8iSsTaSyNlE5m5R6W63aUgjtHU/p8LO43Oq1VH4jo5wB8J4BLRPQogP8RwI+hfAvUewB8P4DPAbgF4EemFIKQGfWGZYjvxjif1PNypxzLnYfQDHg/g5JFvS/T0O3YcXwHzQfIAiYygjBN86PNgg9BLBdbKKiILgPkk+kO9UXE043dxLb+uzgxGeRPkerbKiCPfuWjcz4W1kBrja7r3InU8GcnkIrLnxElkbi/UvYTl6z5Q1VhnbTOw8vnSVsXmHbNZuF3rw3yUuivlMvf5+VN1CakLLCoUpXqhr5NLNfUUUfyEpscFQSkUmqH0vOT0Km4IUqR4kYPPfmAMpwLYeRJSbFRoJM0Yv3IRv1r0NAFHTekE6ifcClTyN/LdErf0/DOZ8AY4/VMV5/Ey1GUoy+f0z+jTmkYSolwwqZgjFm7BCfzG+5IVdH+EE5uyomhkpUBd8aIfbK1AAAgAElEQVSlHoYrDNpSP5TildIBygwj+U4o+jgQ9UyBWTgslW7+Zrf7ltnp5vJt6Zau2uTsj/6Xx+LVHcJCeLliJR2wcoZUYhKZd++kG6JOhUejpAHEh3GGnniEmJi8rCNjiAPdD8ies/oJEu00TUw5kbYQEw7KvZPSVlkQqV6ke8o3SUmGphmAnzAmYyRRenMXTzQKZXMHt5QNpi4eACYo5Q56JYVodAUAEmf9t61GWHLV0d2BwDqVhiVp58pnem+6mg3At1X8kVEq6fwWbk5RWi38WJj8ffGI/orMY3aH2cSzQAvCMZlwlhA4gpLXGGbtFhlMZJAhfUCOm6T8MmvmiJxDXGlgd6PTIeLg19R7e+b7ghhs+mP0por/U7EhKhQ3VxNyODgVgiZ/s+chrfyZSCFKhHQPQ1qGsLzZNE2UxFFyR7WBYb09QIFAlp2XAQNknSWdrHNpLV36IplNLoUCc8jVLgndB9BWfg9lydLMSWeSsKy2yUFeVwPD81p6OVqrxS2htNLzWnlrZRkrt4iVxMs/WcjR/GU5898lJFDuawzGzu2i/1PCFMo0tPincMsN9HoVAmetU2qUC3HClujS4MvLVhoQIV5wQwayK+0tx62tzh1JueM1s3Qk6igNXKI6YyvWNEwAUR4UBmS+aiINpnk9yxMvRVIlcowkHKvP8eMA0rBfajRl0o3GLzDjTePX0IjctrxJyqU2zp+XJr679sAmd2WchE4FUwgDFugHf74LsA+bDhg3YIfbqWuSQ/618Ugy2QmI/gQyrEwjRx1EFPcgJFBf1Esa+WqDr1TvMYgfxkW+9yGMhRTiZzYZcYltHj/fM5GHSctQMHahr0PilSnbrr/xxdlHEHwvbPxdoxQtrUePY0RE/QlNGSqVdVp3mUqtHL3QSPMM7/JnJWSYI6vS2MwZAXkV8yR0KpiChPilzh7AaFFXd2DFcFdkrZHTfNc3WuwMlXZG6YyDHEnkHZmrRYlKUFgalPkVIW6h+ES9t+NAlRKoI78DIj8bIXzk7UnBQ1NSzixqlPdPPoGGErG8D6SW9u1Szng3pdIYIwqrXfU4J8kzVy3uNJ0qQ2NJj3KeggGqI0qhVFoIjpmdpNyfVhw6yYSARQpLe8EgFgaoNgAaNbikRp5tEE5uakmBkTILQ4gGUwsGvKGvCdZhUR6Gcfsrw0qgkO4AwOSWHYl1YmDUNrRhOE/S5RvPfChUWik1WKGQUj6Uv4gIYvun6TGL2nt1xWaTLpFyYcUoWdkJn/FJkyOQMSRWiw+gP3Wuqp/buDJRXM3wz6RwDsze/4p1y47F9JcZD9XYPD2CVw8Km+1iGWWZqG9Dn2ghzpBOBVIABKQv3OsXJFZOueQNz8Y2qNSgeSnt/HvtnAaZb+4f0OebLv2FZ0HHzwdjjpaaphnkmzPQ3AbBItxYG4Rlq3xClWwceVr5gTIldFf6XWo/iZZq/TJWF9kOm1LelreDPmoItFS2TfJkZqjisfDT8ppKp44phO9FuF/Rp1KbQN2CXVIvppRHPpOToMZ8ch08r1uE6qirfQwM7Au3BZODKr9BerUJN5y4BSNrYAKAP1F6U7o70LhEd0L9kLTOvH03867RJmU6NUwhNxCO2hQElXYQ5kajob46fJ6nEf7mNoOcA5cMg8BwdaL4jsQ6+cjYyCdjkKg5gsoRU5IGhupDkmaBKYdPjcEFspy6YRMRrOnApouefEA/2MbaPH049DsYu0kqR2hjSDCPlzO73LhXo5otoVrGyuRkchua5N+BwChc95f3m9yLkqpo05nPqWEKUo8dg/TyA/QnMEv1Y0yC51Rb5QDSVQiXf29sK8HtnJmMTbSoNlB58EbVgihRqUqQ3CVThupBbXH1GUdKNUNgUDGk4TG/9KXUjumgLDOgkP5UqDsVFm9qW7iT6sNoPqXl4kq7l8Ktta+gb+OTqg+nxNC4xroMDUYHt65tAD8BlD98lL2FyMWnOJFyhEBEzhgHAinlzDJMUGz7izSIAM+YVNcBRGhAMNoZ7shYkA1OPf121YbJC8ShXwERQbE/cNWrDfDeAi4/B7eZnalNmdROMJxMDBXQhpcefiM4rA5h/BZtECgMDhW2QAMI+zGsQywmcyDT1t2AbRkwCF6UCkQMa82AwRm7cs5QxvoMkJwdwa5RoL3LlBUsSjr5REMxhhNIIrW8X0u2j9K7KlmOHqylSUrsvVz72iTvgxR3I9EvPSfCiWE5GJQFumOMGA4RL1qOdRn41fTGRBZ7UwgnZwynBCkMl7+ATBXgfrCHZyWIHn7L7/ngqMkApRSkJ2PoVKVUbORgTATEpqDMSBj+GmPcQICAvZmqMQo1C9JuoD4x/HlQbgrJ8wYa3YwODJm3dLRKVLisTgAnJz1JxBSYao4kgH7gy3TzupYk4SYIoqZKTo1/UskKlKV4/uwkCCRXbYD0sCBJuRD8hkYKJSu5lPbut/LziRLJAkBIX8Ct42m4DUO237/gB7Qf5YMyyMHKZEFKgayXgsaAWn9aUjh6jQiKABMmAPdLf6ETVeN3RVJfyXSQDMugBIQtSb9Y57CS4aWqUv5UaSeSQcnR8b2KEQZUZxYi44JNJ16cywh7Q8BDQ6/cEKSVgu1MwixywyybLvyAZM+Bkcg2iuhOMJyucCJUXs+8zSarG/mzE2gQLq+0D/tndea81ve2IOhcXKFmynoG+4O8/2IikzgVSCFiADHg5CToJUA6IOPgAZJGciEJ/aWo2WQUeTMz2PaD1wlzipe+BAlUXnrrtzbn9gfAbYKyBFgClC439Zg0GdP7w998IknIHc5hkKS17lcGQl6ZlJYTVE6q3Mswl2LhGLox3Ty+r4zPElIsMfEpVGIINUQyKMeGqx9jKOBO2yim2EtuB/WcCqbgiJOOkJUqXq6SUd9IQbq57wMGQqkEcJC+nB67L4AS0NTBhGTI5KimZK2vyomoP8Nz8jAJKHAoUMGTMOQT8tVao2kaEABN7tLbRjfu5CQg7tzrulV/HFxoywyhSeQgWrQoieHTViA0uhkM2BzWM4Xtv6G/Q1+J39nHWpONjrr96U7R0B+gLunrhv2eybqm6pdpR6NNSD+MzbSfKmrohgzuVKgPgJdCjGg8rEnJKRwyQCrZYFGiK3fEuVZNhFPkh31vHHLfDXljmziWLDCUaDQUaAIY6t/REEeOAyeTMcLK3ivQGeF6t2ppsGLbJfEDQ2iaJhr1DJmozgABRRKMNQAs2ABd1w1UkWD7iM88HNWkoMBYWhMHYRjY0XsxXJQj+keWUaIZCwVW1rdxb5R0qM4k5WL/zzBAmQFXopFcUORlkOHy8HnY5Df18Vi4McazFeAETM4Ee2bg26V/C8CvEISy+LEWGHGOlkP6BStM/CsNlSG3sJ3aiZfN7CWnAinI4q7TA0vqQI2SASbi6cLVbImrMlJYXrsMxR3p7ox80rBWswUolQ+e0oBKpWtkZsKdOo/bdR2C7UNu6VZeb8/ha6hvXP7lfmeoG0geoYQPCFqp3qBpGTAct3/XdkIGcswLvV0nq3dszwrDL02WWjj5N2/XKZSrPjV7xdR08nLkalqtnLV3pXLcafXkVDAFYNggQ5fg8iafwA1LhqfwPUq3COXTjk4HdZ++3Pko04rlAcAmTCLjVvzIgpQ3zkkGVxgMpZuddMac4nvVOyvJ+pS2NYf9G2HVJKgWIc9omAoqD8NNcs/ckOUPAJopqgmalLNLMJA78ud91JcttR3lfRKelzZYEdEAyeT5yLatpV+Kmz+rPU/GFPUfOdGHtopU5YgoluD2UYh08jwD5fNgUHfl9uNUhaQady8vRlkXgMrXxv11InqMiH7Xf75fvPur5K6N+zQRvWFySTJahwRSmLl5msl3DBt1qtQJtgAZrKSrh3KmE6Wwl6AiYUofmV+eVqk+Y1KW0sBJWrUyye+bSOfbMYLV6E5Kytst35AxyHebpTHJUHqHzSsnvTYOAP4+M7/Cf94DAET0UgA/COBlPs5PENHwsL4C5RARqG9LBnJ317K3nEyzBulDHI7fMVivD1b1PI8g2Zx+XZ4k7KE5ESU2iLyeicqQMQ6JTqSqE97JLc0BwcTSEBKVKKgWebsqGpqptNaYz+dRtZBlTo6hE2qNrFN+NmTeP8lzX5YcoZQYeYn55W0u266EOCdRoa+mUqmsLu/0fS3uVKZaTCeYaSg4xG3ms3DSa+Nq9GYA72TmBTN/Ee5U59esi+QGsb+OLZeuIxK7v/atl4LTrqB3W6Mtd+4vMVgTOmJAq2QgBUNZuDUpwnYmKFLQBOefxwaAgVL9oG6Y0LALGzweS/WTeTn7gQHEh00XGYqsf36wS/yA3cdfkJozFWB4DgID7rg4i3iWoyyTzDeH1FbBtZ2iJEzCTNkC1vS3KYsr+EIbKS4zwtBnci+EvMVqzKZRUjPWTUgRuS8H92cjrkWPISwj+TjSgL8CUJ6pWMw/+11DikCq0gREmjDMDXja7dgU3kbuZumfIn/rNDa4No6I3kpEv01Ev13qorFTinI7Qd4A8m8h38H3wEy01slgC+UIjCH3CQAzrNfD26ZB27aQNglgqGuO6Yf5foLwyQ9Eyf8G6V9btpTh80me2zVku7qLY8ZdhmU5Q9uVPO4kymkwZP7wLadB7j2X1aISlZBliWp9EJ6ts0PcLq1jSHl+tbEv/94NOilT+EcAXgjgFQCeAPB3N02AxQ1RlD6Pf0sTOHyXDkUlNUPGr3nKlfwJZFq19ONk9GqLVgoNCA0RNDViogyleOl4s5BmSeLJCRomvpycuapTmrj5eznJ8/Ty+MvlctAmg7QwnJgBVUk1z1obPRo1AMUMLcvM7KVxL1llWV1FykbB0H75u1Jb1Cb9AOpPmHdThZGsy9gzWe4ak6rNizz+WBuM0YmYAjM/xcyG3ZXGP4leRZh8bdw6ygcuMDxAtB9s/dbdHCqGsOFZKZz8m6sNYxI+GN4bRXjg4i72zrRoVDpQSyimtLRI1KsDpbLKXYqyTUI5S/q7DA8Mb44utYf8LRFAqId0a451FPHy9BOGIKA4Ryet2mrM0CjLXvWQ6KusagzboVSmMZJllb9PTELPPwlJJrGuvrdLJ2IKlF4v/ycBhJWJnwfwg0Q0J6LnA3gRgA9skG4R2oZnAc72kro/D9nppwSqQFOZR8hHnlos4XJ+DXtSPvfFdxDAinF2m/CHn/MAXnj/JaDVYgC7w0hJcRj/MV8p1QKslmqMzFO6K4cNWaQYyt/3ALIwtvP59GmG+Hl75j4QJca7Tm9O3vl2B1KkItMI6Sql4r6HyJR0A920MGCsYLCCgRXt1ffFuPGtxCjyOmwC4fv6ie9w9gLFlOxglOWI3ymcjRBOrXYf4v4TGaK0QWT1CmUfy6v2fmjTWE9TliR/DsBvAngJET1K7qq4v01EHyWijwD4LgB/2Rfi4wDeBeATAH4ZwF/icO3RRAqSaUwPD8/cYzdY3CRLN9mE9HKJKAdHjQlJJiEnjSJKjHeN0mhnLd7xf/4zfMd3vQZNK9UNRliSYu4htZzk+bkMEk0E5hBsHYOJquS1csOLaZJyZ3UrqU4SFeTr46UBGH9Lj8wKjE5VjuGwC2we3nHKZuv71sq2XD/C75YUDfmXlrFvN00A0aU8yWskn1xtvBNlOiXXxhE3UTXwbsX+um4/B9EvOwopXbDmB5JXygUKg33Moi7VltjgoHgmQXzObrK2M42HLu7gR97wSnzh6WP8wq99FNdvHSZuqeQgRfJbQnGZr1QtgvUeQDL5XSIcJ5djGs6bsVvV7SA50+i6LlEH5MAqoakSIyUiMMkdm338fM9KZEyU2ihiGSEt5zae6CTTcAfxFrt80Nfr9PeTUo4GQtqRqXL6Ls9fSm15byTgmIIV8fLxUSKJEtxYQ9zfIvOy+Ea7No78UhURQLa/hBNAv0EntGDgpkOvR8BPIIt4uEB+7kIuGUOc6iGkyl3YYq2NvuSkyB9qoXHz4Bjv/q1PY2Us0HVodYPVapka8Xxd8k7OGVXuUxAlNjGIGG3bunMaLIMt/CYoZ+y03A0s8OHyWPlc5i3bLXyX5ZHMIbcnSBQSLqMNW65l+rLOXddBNX1ZpG2k1PZ53zoz1vjBvDnKvBuoIZZH+Mj0x+wFNYAS4DqJIZFbUu6Z47iKkKgb/nte5/h9IkM8PUzBk5wUNbWBFEUrNJAO6igRFfkBhCRMCRJLfV7q93ISyKIE9MAA2FgcL4GvPbME8xFWQNwslDAf9O7Gi8UiTj75N9Q/n9jOOUgMcqWAzKofJzO7vIOKEjY/5bc+yfqF5yVVKj3TYrhC46Q3IlIIdZHbqIcIJNoY+zRCGSsT2Bhbe5VQKb87TUl7jEF7cKoLTEg3CBCi3J4yHmcdmtiETh1TCFSavMzeuMj97rLBZPdnKHA2vmLjCSljOR24Svkjt/xBKmDXqTq5CKZHNP0FR4zFagVYtxLRGQ8HSeZrvUGubgWPdeB+Ais349yhKeHeSe+eRIKZuInqjI8yLcngwoapxA8jMeileyKUuws9ujnXjrhnawG2yR6UUt8wM9jXKbSDtBUgqBCRCUtpBwC+/zJmk5NkdMX2LVCJgeTPkvsZiOI4KMYd4UVh5YWEBSGcqsZA7GMPkWOYYloRNacMKFwJKJnLVDo1G6KmUwDwtaKz+JTQRPAfKO98VB76KRBg/aUmUpr6PKz3FoRfYVgtl1gZjgayMOnjpCQ3AEy3GmyyIqJkw1LwSAwTDXDMQCsF5TcjNVqjaVoAEG7LPeoIF99K5hPKI12lFSF+CMHdmaEVoW0at9IhGI+0SYRnbN3N1EXIKn47FKR7CcscmR7b/vq+fvJS/4kDvmzI/HpRHF0FFWeTNADkNvFCwMAskQ9rESSyhHp8YKOynhqkILlZaYdkifvnE0tCxhxWxVuYGHFwltJxqwuI5ZATqq7fEVYRprsw4YwDt71axUM7FAis+sNRQjpBiicGuQziy+VCpRRUo2GMtysQYbVaDZyFiJz/g2VGG7aMM9CxW3oNR53lqw79qdqp3SOvf26DKPWHjBP7AxjsBYHtUdiwvxXgb74KbXUnaEyK1vpe1kX2Y2nilVS30rjNIg3KICkYETch10fTFgJPDVJY1wFxgnqxFuBZqWHlgEp8ESCZztD7zf0desSVDJSOgbgJ1DHDsE22wUp3XzmxjbVQsPF4Nll2ZobSuve+IAU2vTXbGRj7ARFPW/K/A2KQzDWuWgBotUajdYwnD2Bt27ZnZL7djDEePfSqSN5msW2zZ/mkzpl1yctUSk4iQqOUQ0hE/twKPTqJN6USk6/RWB2nqCU1YVYyig7R7bAswPiGwTydb0ik4LWpYgXkxA7HlgcqNVw4LSnXaQFCoxWMSZfZjF9VKHFo2bjxsBD0E4RBUFqBO/Z+DQDgDpkN+vKwjM5KDwBt2w4kqlIq2kyUQB8qqgOuLqvVCm3bJogmGPiWyyVmsxlIKayWS4cYmgYNEY4XC7f0ZS1a7wMR8+XerhAnIAtkRQTjD3XJ61VCCiHd0H693aRs4Ax5EFIpfreMhmOU14MxLJMMm1Mf1xuYsrC1yc9sxw2UNFRfnPH5ZOpCTqeDKRCBQlFcy/uGcl6ByXo+U38ElpSwQRqzBlvnNGf98WX9RHYTtV/m83q/h/gcDGkYrrMnMNYbwVgRmAmaGzAtQWhApOLg0RoRmkdSBAsNohBGR+gPwHUsu/MOQ+eTDpYLBa00jDVoG4VgcQjpt/MtbJ9p0K2W2AkeikQwltG2TQyrlYJiv4oDgJQGyMKsOqfeWHaHc6BXjzSFgQznqentMrJ+6cQNaAzJO9sZqEYDSjsjpmcUxgMF6azN1iQ+DYDbL2FQH/S5ygUMJ+GwT1EMK1VQ+W5swqXIz7WDTxnRzsV9WAAwnAowZ98tGHR9ceNyZzCI95kVy7OuzDmdGvUhp94CXd4am0O/PEyvCigopf0Fral/fQLXmKNPREhf6sUDS3SQglrFiePf9GUIhj9QcetzSCdId+MvC0m8D33YZKVBa7RtG+G/tTaevtO2LQjUqxXs0EhgPLlfhsvLqQqRSSoVz1BQYnKkULfvl9wZLA9XsiswM6z3GJXtmULt/lmEyhNg/hiUllST7pvC7XVpjpWTmaO7dEzjDnpL5ohiCp0OpCDKPOT0yqsDApp6pxA5cWXHh+W3Rs8AeBhPyk38BF14NcPb+6WzUe79F/VgoiAuI+t2KxFO73UMQIGJ0TbO0cihAwuthM2Bjbs3olsC7OCfVtrrzm7Ctm3rjEPK775XyjOhwCgoTlpjDBYLf5cDBVXLDTi5k1JuD3fxOuc9Ss4Q1SMy9AeUBjXIL5GF5VZmjn4QUu0Y04VD2kQKxnssKvJ3O0D6RbhMco9LIolgbo/GVJOSPi4dldi3R/QYpKFBPDDE3H4i8yR2uyLSvQnTbQFSqpdOlYjoIi5vrqfTwRSy9d5hw0qo598Fr8JKRYP1XDKZdAOQjuf+EfVGSGDo3SjTC+iDg/sYAMsqwmuQg9+DVQM4I6PL31075+oBjw48YxosuXn47vdCEDmGOJvNQKY/rFUpp04ECUxau/MegWT5NjCG3oXaM0oxeJVS6LouCRdWTyRTzdFUzaCWKsi9rUSTglbiMJbQfiOD905J0JOQVlrYesrOdZuSxeaGwDz+JAp8dgKdEqaQM4L1RiU5CGWDqkyVyJfYwkRwc0tMQD/h2SU+8Pbr80wRitYqeq4x9TqeUj26sNbAgxuAKEprKY0UEVadcQY9P0FC/ZSX9EqR801QAea7d3FSU28rSXZiZm3BzJjP5+i6bmA3ieqOYIxx4luOqyY5cwCQuFOH7qOCgJK+EsH+0peDkzglHX+MSmilZlPYhIKtKU+z9nsK1eKsq2epbifNq0SngynQsNDSTlAz+gRIHQemeJ9LLWOMYwx+o5Tl/u4DSxDXrPUGOCWWdaM0DKsfHNJlhxKUh/UKYMWwsB7aI25WUqCo+8uzIVw6FlvtzDlMMaCU9hPeS2pN0K0Ca38QCRFmsxad/65IoVt0YGWwAuKyppQOcpm0byNnEHTenA4SWwso1cBtcLUwK7d6wdbGzTqaLSwQr8TL+0+FNXGixLuvUcrVRbXOiLiyWNEqNDLA2WY1toDqUV9g4CXhUbL9yPFUswtNZiDhKr3C/Mq3JjO435cQzvsolKWk85fsLOzVNh7MlbICkTOWTdDIqTY0AuMdJhmHbMi8scekjGQypee5OlMzWpFHALkKFNCC/GixDBjCNW6pIkprpRSsUA0CMnC2BDVIEwQ0jWM0Wrmr4XKjal4vV2bh4YhgnKU4+OOKDPcTIl2uXNMuQLwEN6hGClqU2zG0UIe1fTOCIm8Hzud9nLfViahQn6lp3g6qAW4PyZwOpJBJ+BJst2wjPM6daCxzckN0MFiFdHpLu5tQJR+GvBOstVDsrPld1w3fhUEN59wTyiihekifiGCMjQ5OduEkh3NkcnkbBjQ1mDctmiakq8DKoZj5vAUrhp61aPUMq8UKs9kMSnj49dKJYZerRA0KRtW2bbxjkoXWbllxtVo5+wGMv1eTYTrjGEKmSjg1wjo7BRNackuzpXsrpeNUPNEaDbRqQE0DpRW6lYk7K8POkJypyskV1A2Zfp7f7zcFFXVa4GHQsOGPs9fJdYdI1Zl19I3HFJBOynzCMqF3njFDSGSFkSy8lUbF3sjoJJ98XmJG7oG7i1GehCS3D/dQvC+LvE5MCWcSYwxWncGq69B5I2PbtlitOmyFJcS2ATFwcHCAM/MZtre3QQSsgP6YNgW0TYtZ24KYsDWf48g7Ys1mc3TaoFtaNE2DzjiGI/V1og4Ae2aAWLdgUFQNYbFYJn3g/vaoo+s6f8OWWxFq2gaGCqc+C2YSUFvbtlDUYmu+jdnZHUArLI6ugFjYVrLDc4NRMqECvJa/a2MrZ/yprWg4Hm4HghOG46m8SWo4uWO8qKmkjBEALFInvmLKwg7yDag+pKctlfQsa20cDEEqBbdfOfjCgArxazv7xmAiM0dOLNOT0iseoiqWzYD+xKTgchyeL5dLLFcrGGPQdR1u3bqFbuX+Hh4eYn9/H4cHByAidKsVlosFWt2gadzHMmNnZ8cbLAmz2Qy6aTCfzXD+/Hns7p5BO5uBiNA2bXRnlsfAB9dm+WmaBrPZzHs/UvR/yG0eoY2diuQMiAS3HGuEv0GMU2AUAKB1g72z5/CBD3wA/8Fb3oKtMzvFfpBqVGjHms0gf1dTZdZRPk4af0L3pmlJNNs/xGAM3Q6tYwh5HpvkN+U4tkeI6NeI6BNE9HEi+q/88wtE9F4i+qz/e94/JyL6cXK3RH2EiF61vhiMeA8DZx6Agixzwm0NM0zGREJH1CYzWPWfkK5gKMSAMoyGh4apMEDjMWkENOSMei31E+ni2fugvR7d6gYHB7fcZa3KbW82QQrCLx8yQNbBcmU6d0WbYRwf3sLWXGPWNtjZnoNgcf/FC9janqHdmkEphbbVYFLYO3cf5q3GfHuG2UxjvtVCa4JSgFIMpRikLJqWMN9q0baN+/jlUkUE3RCauUa71aKdN5httS7srEGrgVY3ABTa+by/AMYC2o83BbhTrcWA1QRo5baUa2I0tsPFB87j3IVz+Itv/YtoaA5iFc8s1OzSmCkNFdx2bTrZNVG8NaHELEr2gcGIK4yZPPxqtcJqtUouz5liC5DqbfIuW1FhZle3kfmalI0xcHRy52yE8x9F/arIZD1NUR86AH+FmT9MRHsAPkRE7wXwFwD8KjP/GBG9HcDbAfy3AN4Id2DriwC8Fu44+NeOZRDaRBquioeNZLpmVB8KbrbSESlIthAG8AhChXeiA4Uevg6eBn25aTTMykBpDQ3CYrlwEtUYLFZLKEUAKTABrSLotoEmZ0jULYONdVJfN+BbC2jFoK5zdhTlbmlqZxqzeYtZ06JtZujY6ecrUmAo3Dq8hXY2g/B/Rg8AACAASURBVDUr535tLYgVum4Ft6LCmM93YK3Fa17zGrz//e9HO2sxI+3RC6Nt5ugsQOTiuVWWDsoaMLmbswnsDnLROhodQV665BLfD1TFhFnToNUtts+dwyte+1oYAJfuPw+11YL2Rfvb/vxDZ2dgb/BMBULs52z81KRjyVZVWnYt9XXJXlKiEDeoY0XjNaVjN76TttSRyTzFWM7Zu1J+Y7SWKTDzE3B3O4CZ94nok3AXvLwZwHf6YD8N4P+FYwpvBvAz7ErwW0R0HxE95NMpEqGfmJYNCNot7SFskU2NdqHNwrIiEYFtZm0Hw9oViFQ03CjVJOnY3OecvJOOdz5yx5sR3HmBHhKyidfSa68+MCyatoUCYd60mOsGputAWuPoyHkuBgMaBLdvZy14tQK0gm7ceY/mmN2+BLhlTbM8RnN2D9tbM5zZO4Pz589juepA7RYODxdoqIFlgjUWxA3mjQIvV9i9fA7XD27i6tNPAyuG0oSmUWiaGb7w+c9hZ2sb1hhszVtYq8G2RccMkDsZuyOD9uAWFkcHoBWgmhZg4/YjGPij6CzCiVCavDE33LBlDHTjrrF3TH4FsoSL5y/hpS99OfQKQAs8cOky9r/2FFrdwHYdiNzpOMZYkPa7RIlhPHJD5rCWb782KE8E+bsE4UvPJOqsjt0COikxhBhmjf0DCGpZofzVGMP4t6OcbGRoJKLnAXglgH8H4AEx0Z8E8ID/XrslKmEKRPRWAG8d5oGIFoAg0Ye3RcVJHOwJYLC4ds5Jex2/A04PSyQCl9MNNz8HPTDk0zRNdgR30H1d2lo5qL/dzMGkcWN5hIYYGgRWCsZadNZgpjXUrIVuGqjGeR62jca2BY7Y3/Cs3ArA0f4hHnrOc3D23B52985gb28XTBqznT1cu76PpdHorDsURdkVyHTY2jmLBx98Nq5ffRIf+73fwa39fbQzjd3dbcxmMxjT4fzeLozpMJ/PexRFBKUaNLNtzHZm+OoXvoKDboVDa9EysFoYbDUtFryEYYLxbTNrWyxWXTKI42Et2t34pBjg5RIvf+WrcOXgGL/zpa/g8PAGHnjkWfjKZz+FpXGrJW6lZY7Dw8Oo1kR70gTdvmQcDP1aQnpjVHOCqlHOWMZUjXHJnU7rEqI5aZmm0GSmQES7AP45gP+amW9mjcVEY5rRkJj5HQDeAQBKKSYKJ/IQ4NGAUtq5BtsgxZ0lPDgDSTdlt4bPvUsve6dhL/3d8k7mcxA8GhmJhTswgka5JbsICzN4rMit87vBq7CzNcdca5xrZzCNxf71Dq0haA23GqAUWr+ESJ3C1u4ZKFLYv3EdC9vBLhlnbQM9J6y0huIOy1WH+azF/Rcu4MKlCziztwc9m2G2cwaX7ze4cmOBrrNoNIHMCq0yeOjZL8HO9i4O96+jgYa2HVYri+c+8jLsnZ1jcfMQN/YPsN3O0MwaLLBEo2ZoodFsz9HMt/E//+2/iR9885/C4vo1XN49i729XXzpi48CqyW2mjkWnbuH01iL2WyGruvAYSclvPelIpyfb2Fv9yxufu0qdubb+OB73o3f/ejHcO1wHx/96EfwxEc/gVY71YrA7u7MJbDbtlgYhvEHqxCx8xj15zgOJ7hn1qQi8kzGLxzTy8Zg7M8xWJ5/XzfBauqmQ6fCqamWDIt9FUQOLVXKk9ZB5BWYwQnsCpOYAhG1cAzhnzLzv/CPnwpqAbnLYa7455vfEsXOrxzoj7fufQBU3OMQJD1UP9l9+Vwy7Nbj3bN0n0NovLj8phTY+uVKSpcTgXDWgErtGhlyUEpH+0PbNJjNWuzNZ7iwcw6Hxy3ONtdx2C2x6DpY+E1dzNBMsKsVDm/uY2drGy0DbBiaDPbmMzxoZ/gqGdxgC60bmNUKL33pS/Cc5z2IMztb2GpatLMtPH6ocPGmM84a02EGg4cvPoBPPfYEZnOFxx67gqeffgI3bh3gWefO4A3f+1o88k0vxrv+8c/hMx/5BJatwpve+AbY5RE++ZFPodvZxYwYv/7L/xq/+Av/F/a2drE4OsKDmGN1/SncbwlPW4MzuztQiwbHBCy7Dgd+1SQ0kiJ3OIoiwn1ndjGbua3SmhRoRZjt7OBP/Nkfwgsefg7+wW++H91y4RCadqpLywxWCmRWzgsUwdBmexhdgueog4nbs/WX7QB3k0j8lTlGm5j320mfy23aKZX289RoLVMg19v/O4BPMvPfE69+HsAPA/gx//fd4vnbiOidcAbGG2P2BJdJaqQBhlycqN8z0HsOqmJnSc/CPH543/mDQrTWgCn4/xOB0J9ClHoWusZvmsajBY1Z02JnZwvn77sPz9o9i+/4yln80vwAn+iuY9kA3eESlkK93Kap7niBo+UK27oF0GCnUfhudR4vUxfxs83juG4YO1vbuHH1Oj747z6MP/WDfx8XL14GmhZoFV66WOK6cW7JsAZbi2Pcd/kS3v3Wt0ER4dYzT+PShcvQS4MtC7zhT/85NLMdvOtnfg68WGB2YPDFT38a52iGm5/4Ip7gI1y9fgOLGdCgwdeeuQnVdThz7iE89+xlXFnewo0rx7jAwJ6e4RrdwhwKi2j9B8DAfGsOc7wEMWP/xg20UDjXNCAmrAAcXnkKP/kP/xf83vt+E0f7+87eAMacNGZQ6MwCCsAxAKA/ENZZ310edoJuXnpfMj4WDcgFw+QYsshtU3eL4u5MlFBDOc6mzGyKn8K3A3gLgO8mot/1n++HYwavJ6LPAvhe/xsA3gPgC3DX0P8kgP98amFyw05eEbnu7SathqIG4UpzRU36nvq17uA/ECseNuRkzETGlWUKaUWdueC6vL29jbNnz+DFe5fw8u/9btxvW7TsPBeJnMMJk42dR3CrBKbr4sGsL2sv4ltf9a24wC3mDBjvy3D16jMwhsDtLqyag7nBbD7Hpe0d3L+7g/v39rB76QK4Y9z4yhfw6Q9/Ek8/8SjarT1sdwZ7qxb2aIEOFucOOnSrFVTTYP7hr+DZTy7RgKGWK3BncPbSBfDKbZbSRKCuw5EGXvvgCwBNeP3DL8G/d98jABs0CHdh9OTZurMNWWDGCttKY69p3UpGt8Dn/u0HcHztmt+67lUOdku8rWI0AmL3qxn5/8OxUxpTUjhI2lTqy3GxSfhN40kqCbc8jxCuWAbuP1NpLVNg5n/DzMTM38zMr/Cf9zDzM8z8Pcz8Imb+Xma+6sMzM/8lZn4hM7+cmX97XR7BAShUsnQKcQxbaFyi/lCR/B7GMJlDmtKZR/6VuyklamHm6PQj389mM8znc2xtbWE2m+HcfXu4776zeMELX4jX/cGX46G/+6M412x7dcZBXk2ELd1ge6awO2vQkts+3FigtQoXuMWr/8p/gvt//cfxrOMGRlkcHR7i1sEBOu5w8eH7Ac1QyoB45SdM0DctmpXCwfVr+OZZg1vXH8XR9Wv46sd/B8+3Gn96+wL+xn/45/Eb/+v/hu959sswazQeuaXwx9/0A/j1L38C39wpfPtCo1MGB48/ja3ZHEyMB1WL13W7ePLmNYBatATnk7HVQluLhTZAUL38BHbMxKkQ27rBLgGvbXfxzarFGaugt2Z47qtfgfMPPYRZ606hagjYaRQutgovm+/ijNbYIQVF/n5NJbeCY9DHpQmTo4Da2Kk9K03mXNDkeUn0kX9q+ZWolnfutFRCOjltyvxOh5sz+fMF2OlKzBzPwQ8GwxJcKz1LGIM3QEppg+j42Z+1oNjtTIR35AHgnXMch211A4Zz6Q2rE7O2dd5/AiU8/PCz8EM/8iN4wACffNN/jKeWh1g2Cnxs0GiFVil827f8YXz0sx/FpfPncPXpQxzuH2O7adBAwZDFl372F/DFz3wJT28bzDpCR+4glO970w9ANxrACmAFS40/jj6s1Ct86cqXsfPlT+MNP/BH8a+/8hj+yLf9EXz5q5/H6/g83vgP/gu89y3/PX51/1dwGds488BlbC1n+K0PfgCrW8d4y1/7y7h1cRf/93/6NrDRWBwfowXwPNXgz/3iP8S/feMP4V986sMAKfzjz38ABi0MaVDXecan41mXnTFQsI7pweKiIrxhfgG8N8cXj55Cc24PD993Hgc727D3n8fxzQZkl3gxzfBQ0+KVxzP8c3oG+8aA0EEHO1FYPRLjJlIYC3KVIjCLk47LAk2ZhOiH2jC+Xe+ePJb3eNQ7Y+84FUxBEWFn3qDrOqyM37DEjI6t2wSE3r9ASgSJLuRBKpGbN7o3JPqdeLpxV5etVgyzFPBMAczu1oZ4ZwKcM5ImtxwJ7QyKWmlsbW2hbd29C9vb27jv/Hm84EUvxgv+0MuxXCygL53F1dUxlnYFZoX5TgPqLP67t78VH/jw7+FlL3sR/uqP/i3MSOPCmbPYf+YaNFt88frjeOWnzuCWNlAGmOkWSrX41ff8Ip516TzuO38Rz372s3D28kU8vk9odi/BgnHz5k185oO/gc++95dw5atPQm9pfOebvgfvfu8C7/vQZ/Hpt/w1PLa6gatPLfGlrTmeeuYqjvQMXzpcANst1OUWT/zTX8ErX/IsbG0v8LVbS7S6wYWlxurJx7FlgGtY4GC1glF+2zQDBu4OirBpzDLDGAvVOrVh1yo80u7hJX/9v4R69Br+4Dt/Ah86fBqf+vmfBjcNzpgVLp7bxvFS4S14AN+yvYO9t70Z1/+nn8Dn6QiNcj4ncRVKkfvud1i6FSu/IgH47w7N9Z6jQb6snzQ1OF6yRZTi+R8h0iCNqQyhar/gsTDlFZRvSKSglMJ8vgPmY3S8ih6bmtwOPNvZIiOQ8QNDCBNaaQ3tmULXdVCkcP78eeiGcHBwEM8r7PcwMOZ+kkefBGiQZcAyjDXuvkZ/WObR0TFu3TqCUuSPQTN4/PFHsX/tKvjx6/hD/+R/wK2z78RS+SFpGF23xPv+zftxcHAdH1rcxLXrz8AuCVcXz6AFsGgI3/bGN+FlP/XfYLb7B7DQFhoaGsDVK1/Db7zvfXjo4Yfx2c+dh55v4+n9FZjOgTVhf/8mjq4+gaMrN4EV0JzfwxcffxTPu/QQbj2/w1e++hVcP1ph0WmY60dYdh2eXnWYzxTOdh0Wn7qCZ4730bVL/Mm/8GZ8y8tejb/zN/8eblw7wPILX8GNMxrdYgWjAUMaHbvbnsJSmVvN6W0+W0Q4N9/GDIQ9Npi/8H48/rz78L3vfBCfVk/g3KVzUDOLG/uAarZhnzjAsuvQrTSa/+yP49Lf+SnM9oGGFSysU3RJocvuLnDIoV8iDudVTJkGUyZMTXXNmUeSzh1EKLez2hEO/6EKk6rRqbh1entrzi987iM4ONjHzf1jdyAKEWYWmCmNazCDE5Vym0BUBbzzy2w+A8gdVnp8dIRGN7hw4SIYHa5du4Zr164NrlHTTNHwBTCaZu4GjvfBJ+XVDG9nIL8EOp9vYW+nxUv+wIvx7X/sj+H42iH4l38Tv/zU5/CM6XC06NwGABh853e8Foot5ltn8Gv/z/tAK8b2TGOmG+wYwhtnD+LcS78Jv/SxD+Lzq33s7p2D3prh3MVzOHfxAp793Oei3dqGVQ32jzscM2As43B/H/roFrrHngSMxfFui2/5o6/BzatXcfPLj2H1ucfxye46dubbYGuxMA6JKOXg7HOur3C8Q3jk1Q9i1jaY7c7wtSeewnKh0H74Jj63x1iwwlM3r8EqBaOAo1UHIp0cGrtcLNA0Dc7NNL7jzEP40tEVXFQN3q6fi64BfvbyAa6hgzpH+Kl3/gS4W+BH3/Y38JmPPwosV3gJ5njT6jL+1dFT+IXFMziwhCOzhDuGhbAwFgbsDmiJzMi4PSRh8Cu/0zIbZ5swgbGzJtdR9A2w9bzu1PLmJulMvXX6VDCFc2fP8re+6lV49NGv4vHHn3Tr7rbD1myGpffLl/0T12or667s9Up50GbsIKqcDE2Exh1r44IRgVmhbZv+DEHvnYjAgLzNYzabY9ZYXLx8Ec9/zvOAa0e4duVxPPPMNRwy49iwX0Kz0A1ArKFZoVWELU3Y291BYxnLw2N8E+3imjK4jg6H5gh7ly7jyKzwwIP3Y/f8WZy9eBmdNdB6hmO0OF44xnZw4zqW5hjH1666TTbWwpCFsh06MMzBLSxNh5bc8unB0TGICDu6waVzZ/H0E0/i/KXzONQL4HAJUttYGQNtgKPDG7hFjItnL+HRq0/CWKcmLAGYLr0t2vmPEB4+swvNC+wZAjHhdWoHqznhyuU5DuaEszt72Luwi+Wqw1NXbuLo6BrmByuwJjzwNONxAJ+1t7BYAcdYwlKDlTFYGOvUSiKAnD2HrQVLBLEhU6g932R5MYnvz66Qt42ty7O2BLpu0v//likQEROEkc9PQNkpwaodvq/r3FpnMjMoLHYXSMFt4JFLQS4td/Oz4v7AECJ/k5NWOLPdYndnB1tbW1D7x1isjnB0tMKKLY7MMjnnQXv/ilYpNMZie2sbGox5B8zUNjo+wlIRjmwHNd8GZhr3XboPetbiOS94Pg5vHcFQA2p3cHxo0TTA0eEB2C5x68YNtAo4Wh6DdAOyBnZ1jIYUbt488EfdMzpj0GiNnVmL7niF4+4Iu2f2cPniJSyOF4BifO3K02iVwuHhLRhr0FkF2m5wsDiGYQXu3OU3XWC+3vNUg3B+dwdqucCcGdvG4uLOHjo2uO+B89h94CKu7x9i0S3cgS+LDqvjBWak0BrG1YNDLCxwsOxwZCy6FugsYdl1MMzobD823H0Y2QQiBVIKXWXbfD5e1i3rTaWTTOKITMR4rC0fynsfppKso2E7iSmcCpsCMDSq9sbk8qrD2LrzmCHIGaY4QREyLCMdKP1uTWfICfERB6DbMBW+d8YAyyWOV8cwUOhEdzP77dnan4eoNBrVAIqgoJ1EXB1gSRZH5NxbsVpiPt/GsutAbHF8vMBisYIhAzIKi4WB6QjdqgPBgGBxc/8Ai9XKHYKiAcUGLfW7Rruuw8q7it86OoKybpvw164/g/2jA1dOMLrFCjPSsLZD5y5/xGrl9kscLRzD0xCX5YaOI8JquURrrbuNW2k8szzCtm5x/WiFgyeuYn+5dCtNYKwWK1Bnwa3G4a1jmCU5L9BgR+iWMGL1XCkVNx0xEM/UjOMCqZSvLVmWxs4U2gRBnDSf4KRUe5efrr0uv03qd2qYgiTX6OUJL/+Owb7x9N2BpfDLeb1LaTgQrD+dqT/Bid3+foI/pFXm71DM0dER5rOZu+6SGSsGOuvyc373zgvTGHcsmm5nuHzpIh568AF0iwU+/+nP4JAtYJzPwbJtoNsG851t6Eajs8DBwSGWqxWg5+iWR7AdcHy8AhkDNscwqxW0Upg3LaAUYDtwZ2E6E704LTPa+QyL1RJbSsGAQVqh8fsywAyjCKS84U45JtZZxsrvGPXeJTB+GTm6pSt3kYxbxWnRLVfQ0OhWFtu7Z9C1M9xcLHFrsYRuGpACDo+OQJbRscEOKZAxMEToYOGu7FCA7V11w14Y35mDSUrkT3DKdiuOrhjcJuXjsvR+KtR3iIBqmoc70NeUVec7Qafk5KWT0TqYVg/vB3UWpGSz7jvbMwpVazLX6avlEnaxxC3tL6Fh8vcxOPZj2bnoWtKAUjg4PsL+8TGW1mJlVrjFKxwT41gTdrZ38dDDD2M2n8NY9kbTYxwfL3F4eAuL4wUUGGa1QtcZHO7vIxzuulgs0C2XsNbArBxj6PypT69//etxdHQEZsbx8TEYQNvM0P5/7b15sGXbedj1+9ba59zbw+t+T2/S0+BIluSKB8mSSRSJBIpyEqM4c1WKsiuDMQo2EKocMCQRUBSmkipSCRiSCg6GmJgUxAmOIYoJMY4tQ4UQ2Y4lS7KsydZgydKbuvv1dO85e6/18ce31t5rr7P3Oefe7n59n+t+XafvOXuvvfYav2l9g/OYA7ignT3TSE5im2NfQuiUkE+E3Hisc/xHTc5qzjeslw3rxvH6eJHDux3t7bvcWR9zvF5xfHzUIyq/iryxeYTLyyWdV3JqORjiY2REnSNS7Z7rswH3EwHlGBYPCs4sUpgTH2ooLcjqa2OdABt/62emFDxTzw9tLKlQjqXoWfgFr2kPNsyoc10RaEPHrbt3eOHFa3zyU5/i05/+NF2MPLkOXG0dLUqD58LhIS+++CJHx0dEjRwdrYjBgqt2iRLmsHQaIzF0fT6HrguJzc5h6W2cPv2pT/VjFruUyboIOpspbW8Epor3makcAtu8+plXj+ei7yO9mOKbBU2E2Hj+pfWjPMkBQcCpo113rFZ2giFi2bb/xeXTvPvy63rjAjvZSEi1mI/c/l2brVwD9acuty/ci+hwEsXltrW/L8qbW7vb4AyKD+OjoF2dmRrobXJUuaj6a4AkLkBVEe/JLrian4nBKH4IfbwF1RwlyE4Ulm5Jt1rjXEPrPTHcIaaktuaC7S2adOiIQS1tnHaE2NGJ50A873rijXxm2fKV536Na+1dLh3fJnYt6yNheeEiq9Ua5x1RHCodbYDueI13jq4L3G5XRIW2i4gE6CIHnfDu5iIfvnLIszdu8tnPfwnFs1oHGhHWKcqSZc0S/MJDjFiCXnA41l1g3QXLmqHgnXDzxo3BvVdN5+LE0XgzBdWoRAdrBKeOP+8+gbtruhJ1ltNCY6RTG+uXvPKXbn4MxKMidMHEFRWfAo4plgfDjRFEtUWyC3Ymp+X6mNJDza2xcp3UhGJbHZOigiSxQBQtabFME7Sh3a64thlyLSP7rHS1Vw2naJpMg0/COJ1BpHDvMKdo3IV9h7PuCa20mj5BijpKihOjye0xBI4Ubq+OaEVT0thk6+At2euqa1N7SLJvx8GFC3Qa+bFbn0HWEJxwdHzMV55/gYMLlyw8fKa+NLQxIC5y3IrZBiTlGzGkd5oVoCfw2OKQf/ep1/NHvvzLhGhRkbquS0etCkG5sFxi6oxI6FJ8yxgQlIVv6FJUpS6EXgV2dHRsbL23MPElVRI7MbQ+Jtf3JgxadhOnTKfj0vMhcTtmDOLoYpcicGGbKg6KxBHVL+awnOOM5LedSG0LC18rruf0V1PK742ymhWi23dnmcN0Lyjfm9UshZ7sNHB2kEJhL67FzxNXc0I2sJy8XQtERPBNYwuXHBXKKKnIkBNx1a7Bi3k/OlvEzpt8vFqve7nYO9s8Fw8u8PQzr+bm9Wus9ZBbt+6k0wnh5p0jRI4RUS5cuMhieYBXEO9Zr9aowKpd04lFgdZookNUxcmCIHBxeZEfe/Y6d46OaQXo1n2fOoTYBtYS8OJYtytM7e9NJ0JEtKONnZVVW3QagtEtEQ6XC9apX+bJKgQiIXqkccR8apPECsWhSUeBKFHFuAFVXNMQQuytGB1mRarkjTKB7E9paNQ/sYcCcB8l4bbTr30X9DzXMm+QuA8TYKdm+7XhDCGF1OJyQPtL24+Spli3qXJ1mW3HllPlbbMNZWsFWwyR2La0wY78TPY25tYvFgPLmw+ck7HU0fExn/3s57h68QKr9QpxHl9Q1MViQdu1xBA5Pl7DusM1Vt8ao/hBIYQOgikUnYPF0qNR+JU7z/HZbmDpQ+Z8enB0CRMHjcSQfAxCMJbXNahGggpRh+PH1EKOj4/7+eo5JzWnNsUQhHeGBEKMON8QYkAEQmgJyeFNo9IGxXl7PmqEPss2qbz0Ebdqjm3KmG0bqz+cOpnyd0602Pf6Lm7Untu8lp8t0xHMrcWNOgVs4Kt2oONgsCeQH86MonFjMPfsw76a5ymWf692kBdkcrIqojHleprGksas1ivjFrJXXzoJUBlyVGSfC0MwMekbGpxreOLxp2iTn0TGPv2iUNu1zllG6NVq1X9CIQ50wfI/vua1T7FcQtfdxoUlbumMKhP7TZVjW4pztomDDXtUs1NALIxcl8qwQamHzFjW2MRFDNic7JyUwfkGyMedFsJN0sIO2qHa9WbTUh395nkpc2uYmDLOGDY3l7P6gy06hfxcfS1/r0WHWcWeTq/VqbYNf2eLb4V7PXk5M5xCrWARdkyiDJPuZww5piZqSnnkismfnlRzX26axsSbTPExf4mF93hziiB6Ow1Qr8QuuV9XmNwURCQW32z5nTqeffFFRIVOYlJKDcdvznvUOVO8uUQ21SitqS0ExdE0gkTHFz7/LL7xiC5oXeRSXBBSNObRWKmHAKKOtessarJPBkneWP8oYu81fh4RiOqJIaJe8MGBs+CqFy8skBjQYP4RdJZ+zy2M+xD1BBxROiKO6JKiDGhkQfI/szFO9h+uGZL9GJItsl6ZqqYP41fOeZORX1Jc5sjPPeeX10Ehx88pp6f+AsnUJacuzJ6aA3czqi+Fqc82OPObV4ZPHPQDkxaNbp6TsPKFaLWnacOZ4RRgGjPn3yXWz3kB8u+o8yxbWXddz3CCMH5/3R4YDGdCnR5NLBy5OkdzsOzl7BBjHy8yB2jJnyFJTSSRV6Iqt27f7ln2HFI+ZylaLpd471k0vm+LxZu0Se+6jtC1FskpHUNmn4C2C9zq1vSWl8VQZUezjITKrMaWd9NEgBjHWbyiQEibM1uHigjf+d734ovsWKJK48ScxyLGrYi5V6dCaU5tEy2cY5n0LSrGbdVZvzLV7Tepm6bWpYjj0rtGz81R9ZNAxaLPiaTl93oN3tP7HwDcS4ao/0xEviTjEG35mfeJZYj6pIj8q/s0ZONYZlubinKZYk+0e8QRzMlpmWIE3cxMVU7eMqVk02JhCUbyY7Soxrfv3KHtOtquIwKddiR13AgpTSW5cS4FqCXFhCgsBNu27SmniEV9Kp8bxk5GFFVEbGM7oUUJwbaHkt2Nc3j8fNwnifQ52qiEYAq+Vdv27crjEkIoNp8pNhX4gb/+1zk+XtP1kbYNKV5eHtIcLNLuV1zjiQLS+ORBikWD9oYYmmI8eo6uipiVx3KKoxwh9HRa0lPcAkGPiE05bntu1K3yfh7XTPQZ1tz9gq3vr9b/vnAvGaIAyqfh2gAAIABJREFUvl9V/3JZWES+Dvg24OuB1wD/WES+RlW3ptnZHKh5Oa48l86bpSw3h7Gn5bZN0aF+rkRCi4UfK0DT91iw5fZuHbGQeaG5JL9PBd40WSMtdujt+01PYZvEucYUkUVYuByh+vjuERSIp0cQmmwGshiTmIVebIKeAxLvCTH0TkZZx+Cc71P05X5k1/OAxVR0jQcJdLGlcUsUaLynEeE18YC7Dp5ddLTa2bsLERAnLMSxAC41BxYFe70u2jsoXkvO7jQUd67cSbJW9+VkvIacc+SVXnOgWSSu9RwPCk6LfPaJ0fhlVf2F9P0WkDNEzcEfBH5EVVeq+lksgOs7T9W66fYMfKECk3zCCeqCXq+QN3N5LxWg6yzZydhTMybFnVFVyTKgSAovlxSEms1+rX5XIKR+c6DgwHux3AYacYIZOsVsMuwSuz/g18PDQw4ODliv1iazxqH9mQsIIRKKZ0p2PKS2qTOuokRWmtqYxaFyvHL7nGpSb2TLSkVznk4TsHHAa90FXqcXaHxWEi4whWOWvx0eYalCE5VlMkPPowaDmJjFuKCG6MrNN80xpI2YPxv3h3HZF0Zcaq3lry+W9/aCTFSGNTNV344Wctq9cSKdgoi8gSFDFFgo94+IyA9JSjDLfIaouq7vEpGfF5Gfh02Wbpu2dpjg4nivqGPumakFk+tyIojGjUUz1TYtF48kgx8jt8PJKjlLEiOk4Jw5XYmYRaMtLmeekl5wAl4ULyBENFouSCMzFvk5po1vCWYXtOs1qMV6kKrdgimcevdixuyxElFRuhhMqZiP+8jsb/l9UNAJ1n5ELR8DWV/hsNRzEEVxGnHAUgNXEVw0rqMLgHhDIGoiDSlOQ4xKjucuxcbXxIqrap9AVWXsFVlzDzb8Mc3teH6nRId9WO3RM4X5eH8vJa7ZZyNPRlpO3J30RMo+U2WnozUXikqdfm4b7I0UpMoQhSWOfRPwdiwl3H+5/2tBVX9QVX+LJv/uDQVQrbmF0f37IZeV8uPUJ0eIzu8yN2oL9Was9YAosn8lqixwHLiGA28JYcvo1Dn1+2JhiWIdTdoUjqaxgCGHCAciLNTMo71vQM0hKosUTdNwcHDAarVivWpHSskyffpI9lftHaby78wJAISYfCVSmra5TVPPRf5rmzNl8NZkeCSG6Foirz5cWmZt57lw6RGieGJK9RdTEteoFvou5FRx/RoYt6NuX75X9rds3za9QznHZZTmuo9Tz2VwyMZnboVO6S3qtbcN9i0DbIjW+8CpM0Sp6rPF/f8e+PH08+QZosbvyt/IbF85sVMwpw+Yrnd4pg7RXZcvKU2/OG2194vStOOKF2vDolmwzAhNhDZ05u2nikvIIbOpXWxxalwBCGHtWIpnISS23riQGLv+yMuJI3aWsyHbKbTJmtAzhJYrxySLPiIp3kPuc1J+iaaUeIkqh8Q51Yggj0vejD7ZZ4xY6XSa0qlyoMLSNyyd52Os+OyREr2gvkGWh1y62nDr+g1COMYnJWlEWceOkDKCFy+2Mczm3G7YdGU7sxJydr306ptNP4aTbJx6XOZgg2up11lVzq7Z2p97R9TdEaEH5Ll/WzPsc/ogTGSIEksVl+EPAx9L398PfJuIHIjIG7GU9D+76z2bjR4P5nw5NrT5+8Icq9jLrXkDpTY0yWvwYLHsM0+rGhvdy6NRcUFZICzFHIfK1d0bLsUUM0jM/0Bji7ZrljGyiOAjNCpIOqcOyftRRDg4OMCLsF6vLUaCNbqPl6Dpex6/7GKcoy5tLMxkbVgyvLnMIGePFXq1X8EG4khHmbe7lpVf8S9fvMLn19c47lrEOS5dfRX+4CKLxcHwrBuORNuuHWwqZCzvl4hpCpnPtame41F/ZriBup5t1zbKsIkM5riPyedn3lEjhNoh7F5hH04hZ4j6qIh8OF37j4BvF5G3Y33/HPDdAKr6SyLyd4GPYycXf0p3nDxkGE0UwynDFLatFUQ1xp8d0GoyclZrElZNqBWg165njqXrAodNw+GFC7RtR/aQxIlFnlZNjKOJEw7LEbFOtgSZte3lX0zBRoSFa2iASweHLNVxd3WEAMdpQ5j9g7H/IQS6tqUbHQvmwwvjBuqkOIvlgitXrnDt+vUe6VnTbZxzwNpeqRcHk1zTFUgyTbZxynL8eAIZKDF2yhKDcPtO5HWXD3nThSt87PYNmmaNcx7xDYvlkm65gHWbokFHxDVINCWsHYuW820JeULX9rYceWzLo8pyTUmBwMs2lmtnm6h6Eiqb39lzL0zX23+fqnvP9/VraYZr6HVKJ2z/TqSgqv+EaTXmP9zyzF8A/sJJGpIVIZpDa9mO6alXllnHR3m5s5vNm8PEk1g/Xy84DhHpM/fm+hWljYF1u+bSxQu8dOc2fY7PJFXYaYIzw6XOcOHgTzBmbR0CGnF4nAiHzo7vLi+XxBBotbU4B2InEuI9XdcSohKiYgpMV2TMDha/MCkMS7sCDZFbt2/jvTdT6xxcVIex6sOji1F5tYHo+44MIdRLRfBAaYsxFwUCXhYsafgavcjq4pN8cnWLW3ducHDjRVheNEcqzKZjKZ6FNxdwqyuOtpQKECNBO0BGogKMicNIjMrBWorNWp4ilTC1bvZBDKUI09dRYtmijaOjz4llqlK0o4gIPYXw5qB//hRMxJmxaNzGTu0zcSd9V/kpWdH8ySawtTFQFyM3bt3i7vFxT+l7pSPJWAULgLIm0mqwcuUGLdnezDljrtsuKN3RmoV4RJJNQ2pj1wVu3z3i7tFxcfyXja4Gm4KQuIC8iTNFXa3Xya/BxsCLZc6u5c5MXUdjVtlVlOHXhmdzW20PhhAIKHebyFHb8Tsf+yoWVx/BX7jA81/+EtId43y2/nT45FG5cClCVSEyxCzSYVGHImXfGbW91qtkhbD1g42NtU0ntS+VrRHk3O+yXSddxyfmWE5UeoAzgxQgD+y4K5OKlsrUuK5jTleQ70/JefVEajoHV5HBkCdR0YiaezQDdcrOUtFBQFm1Hatglo0hhHSUGCcjUrv0PqIS1y10HYsoHLoF3ufN59NGFdvsaQwsEIrFfcwIoKfwSfaPahyOCrRJ15BZy54gFVS2nod6fOqNNOqLS1xSzErBDrdu+aFbn+WZ7oDXtwdcfvJJDi9f5Nnnv0TUNtUX0BC5LAuuiCN6sVDp/VymTY8jqsO7wdw7U98pfcI29rqEsny5fvbduJPPT+k5K0K0rb6ea0ufXueys0nJxuGESCTDmUEKc8qcOXauls2mrs29p9ycc1CzoflZx+CmG4s2dyG5MMfIcRcJLh1bJmvEMilpP+EMi8dZVAYOfMNj7oDL3gKphFD4FjgHMvhSZB1F1617/4VkMUDQSJcDrmQHKrGNJt4OzFySxaf6PbXIa0SQF3d/DKiDFaZPnEQUWLnIJ2PH937lg1xzLY8cXuVVz3wVhxcfYb1u6TM0OMGj/KaDSxxAf+JSiiaK9L4hquOEwDCO5VgiiSmEMUUMTgvTdUy/735wCvsghtP258wghX5QYxJHM3etOmOgMUC9cOtr9luw/e16BDpFVcq/o4lz48UX80aLSozQxcg6xQMIorTaggP1ZpgTxRnXoNYGe4+1a9k0eCc472iC8lizhBhYucE5KlphUGchz5YHvTIxxoDqEK0596GmohnB9YhO1cYDsWNTyoVnAdBMbxNRBjPuOUqn2BxpEmGiRpxEXHSsQ+Dnjm+ybjtufeV5HrnyKM+8/i0g5kQWRIiiLFS4FJXGLfow5v28MSh98/xk5Czeo+I2TqKM6wto6CxEW4HYonYowU6BqDbdKamsPStkjwsp1lrfbpPzzPq1WNtznykQkS2Iwd5fchq4/RHEmUEKsKmsOQ0mn+Iaynt13WM5et6AZFOplk8FBmWkeTgO5+tZ0zC8Y9yOgc03nYP3nlYDh21Hg3C0Ou6fiym0unMuKTLdRlvqvpQc0RTiy7qFoex4w5UbY2oOppR1eTycOAvPjutPOLoYuXN8TAxrvvxrn2d19zaPP/EqSNZ3aw2sNXLkIodxzJ1oUlTUiD+/q+ckKqJQXuvnZAvrXt/bpWQ8yZo87fNTsEv8mCq/L5wppFCCQLJ0nR+002PzYQGdTME0LpvZ+t6luCyT652SK2HQD6hFds7scBOVN8tl1JtIUdrRZ9laNQ4RnLeworkP0zoYLU5XBol9qg4TDTbHqjb+6k9BSkpOMraRYVxCaCF0dpIQAy6Hj1doMc5r28KcipK90d77BPfCMWyDDaJTfDbKPpgmzMKZQgpTiqttLN3cRtilaDTY7Prcsz1VTSwqzlu+wp59TenPXUOMYLGPkoJyjCqsfmf+DtkAytysTdH22OKA3/HYV3Hcrvu4BBbnX/qsziGkmAkyZErKys6SA6rFhmogRgZIZSo8a5P2iq2IWv7GKbGqGNfsG6FO6FTSaZoQCmXZuuto1yvi8Ypb115gdXSMSMo1oXBbFRcdK5c4nIn4FeVcZa/TfowK7mIbhykiCB7UjJJzneX90ygcS8ih6bMY4ApxYqM9bCIGx7BKp547rdHeLjgzSGGXJhbmkUC+t2viaiVapoxzz5WUuHxuxGK7rOG3RRyMfzA7gvSG0ZFYgUhMLLDjy04josrXh0u84zVv4mk5JJ9zu+S9qH0/rX0hiQflAlbVke/DlLY7KzfL/o2O/6qNuE2PAOPF2UdrFhuHTi3XZKcWFGbhF4hC23aEtqNbZS9Qs4Jcq/IFXROKdwMbR4nlfPfXJ0SHfWBqfd0/DmGeEy2JXyw++7z5pH08CVI7M+HYQLB0btuRQ6kXqE8GttZeDMrw3ZMT2W4rn+s3RVqxMJPqK4pRgb59IohrjEqAhUr3pbxvEx+TrhIxMaEV+Kjc5n/51Q/xSe6YgjIFSw3aWS6EruuNrASKqMgCCTG1bbsxJnkRLZdLVqsVJNVh9jmwTZZjMFjfKPQe+a9AYeeQsFb6HVR7qpvUlmZIJeYQtegCzbJhrUrXrfCNHe2qGkJwqnQCX+qOuOCXoKZrCWwi5ixCDDqThKQZOCASAi1FIAcWuJbd+oJ6/qfK94hW+kZks6/ZOvLICIOycMP7MY95teZrBKZ5MRViZolWSq/dfeHMcAr7NntOlhzVtAfXsLUl2ya/+GSjnz5oSnpn7+WXmEERIYZICJZcpafegGsGw59jDdxF+Ud3P89BAL+OvRFPFmHS7hy8+VI9vTViJQLVFCUfQfZ9KPqWfSBccdJi/UonFEj/F4oYh/07sqfkMG4hmS4HYBU62hg5uGBcUNvaCY3rk7s4S7QT4LhrBwTgfRrPYW7qY+XeaU2k//TIsvrcT53DCPas20SXPduwk3vJKKiur5zhk8EZQgr7w0lkvXymXz9X/q1PHOYQwga3UVAhTWnXaoRi8rrDN753xQbzYXjb297Kk089TRT6BbwOgds+eVG6QjQa2cwO+gJLVbdpLVm2oXQFLl2LM0JTbPO2nSk8R+ur1w+OxzmHu8+sfymmiLOxiZjbNOppo9I6ZR0Cx+2aw4sXERwShxB0Wbo/PDyw8RTQFIuiVC5Ozbn35ola9/00YsRJoF5b2+qY4j43ylDpnwpiNPf+fRDRSfr2ikMKtYxf/q43dcl2ZRiVdbJxflsjiCnoN77SU60y8WzehJHE3gk9wvC+QVzgrW/7Wl71xKM89/zzqVJYqEe9xwXh2Ck+sSOZvS9l/a4bckaW7a43TkYEMHOEKCkSsnf9904Hd+hMxafGwPpomadDVqpJThhj0KnZKKhCQOgSx6NRODy4gOhYyWd9tRgSmWvJ9eZyUwq2Xryb0Tv1n+r6vUKtu9kG+7yvVzIWcR1UtV8/U++/35zPKw4p1Bt8auOXkNnasnz9vV4oNSdScxd9O9Ira0pdt8/7IXtyjIF3vONf4IknnuQX/vmHzMxETCstgERFvWORsixl34cYI75ZpDpir3EvXbznuJ2SU5hq4+Z4MVLaTSHdWp8juQ8Z2WLDrpjeQ3IWrRhpW8tbkZWoeaPHlGKvbdt+E9Tc15TSdJM4VH2ZgLkNdlI4icJvimiN7hffSwKwbeNnncm2d560X2dG0aiStOgxT+zJjoNKMaAeJJGkCMoa/MyK5vonni8p0iSnocnLsWyak8Q9ZJfrjAgG0SKq8sVf+zKf/dUvsD5ucTGr5gSJgZuLNU92DRfcAqVDUnIXxXFpseRmdzfHVkvjlcapkJenkNiUyFX2uT62bHvF6cDliAbMMjT21pSjzYlZ6FmAlpAovcVe7FQJHXgPjkBoj2glZ7o2ZNGIQ1XoBARHxFm+hhjoxEYhK0br+eg5DcDyLgSQIldERhqJo5laP3MwJ1aO5n2ivuztuDHuFLqorAjMRKNCyuN2eKAWj0YNKcrquL5S+twBZwYp3C+oN0I/KdVGqKFGJvXGqetzhR4hxtibCcM4z0P9LuccN27cMMoYAqQs1qqKOniEhkevXOH5l64hzkx8Iyanr1MsRkkIoG57zVpPtXuK6mbIJtH1WPYKv7SqMkKoqXYv4pCQpnNmZ1B4XA6BayK6XqNqvhMOM17SmJWxhVghdopQRpaaQv5WNotJQzyI0TODqn4Euzb+3HWrc3pdScGhTr1rqv7ZfsE4NugDhDODFPqBSh52o2tbyk/J0FM6hYEqJo88JWFpHW2wEmrf95qNlgr9mgxu16YoBDLIiuu1RV9ukpLOARIjjy0vgAjRWd+8t0SvEdMjOB2oUN1/TVYSIH1w1xq2c1TjMS37ngO9eIcFak3Bqklp3yRxYkaNNXFNmxwLqR++sWAwThrbyCHaEaeYDwSJ6+gpLvQu1rk9dduzoVO2FlUd61CMO5wWN3eJAFPjNTV2uS4R6aNI1eO9rZ65UGsZ+ZRtmUdkJ7NhqOFM6xS2dWxOltuK0Uf3lCl+qtY5TCKm6trYU29G/5BuqSrrRCEhbTzyKUWKwSiWfSlDmfBklyBl4zI8Vy6gWkG3a3xzHbkPw/OpjxvjVymBU8nS6nIYlKyb6IgxRbFK1yEdZTJsrnuBeh7nNtM2IrRrk51mrU7B3keVDxD2idF4KCI/KyK/KJYh6vvS9TeKyAfFMkH9HRFZpusH6fdn0v03nKRBJ0VwJQtbiw75fvm7VpLla+X9cnFMsqtzlEbs6NABi3RUWJcrXZWzXJnrW/olx23L9Tu36IZSxGQN2G/uimMp6yv7UCqrsq9EL7vOsKL1WExR5KnyGRkNsrv2G7zUW+RxjKk92RI0i2EZMWTv0G1tKvsyiBkz8v2WtVFfn4LTIoxZwnJC2IeTmTN7Pun79+EUVsA3q+o3YuHc3yMi7wL+IpYh6s3AdeC9qfx7gevp+vencvs3SJNibQ9Wq49GXMnBNghmITm2IDflYON8HxfBrsbEetvH/LfHn54FJADmipvH2kuKyxiDhS5AaFAOULxEGrcpu5teIOt/Ih4HwWIyHofIKgY6BJyi6iyIqZgfQZRKlJGIktymdXzSUMevzBtkLjZhzeGUHINRbSUWq8ZMsM2HoGQcovOgHpEBOZaIwZCBfWLszGRblECgc4Lm7N5u4NhKXQ1eCA4LxuIH5GuCQ4XIsDYKFqhmxM3otO6nHpNdZcCMsEpX6ezzUD8/QhTJv6QMoDIKrFL2Q1P+iuRy3WfjzW1MeUckr22hL3df8z6owe30c5E+Cnwz8KPp+g8Dfyh9/4PpN+n+75Q9UFWJzfsjvj3UpXMsdc5vYGzsINeNWPGRrmFPl2uk9/NXLXIZ9u02U1qvsGg8TRXurHxHzPJy2uQhuUcfibLS2Dtc2VgkKjmFLNVRhk8rN1+Zf7F+rv5ej0HuXzk/3jdmnlshj/I55wxB7DN/uXyJgIyrMavQLHrk67Cp6JyKoTBCmlsUfrvmviy7DyexDXlMEbopEWbu+dnrc806pVphL52CiHixSM7PAT8J/ApwQ1Uzl1tmgeozRKX7LwGPT9Q5yhCVrpmiKbGUTjbZ75GooDryousVPKqj2AC9TTI6cjkeKmXyHeXvgaugT8FWb/QcuPXAey41CxbiaSRr5ABnG3+oN7lcS9JgN55jUa7pirsE1qEjH9rlOlyx+HIbvTcPTY3jI9Xy+1yg0rqfcwu/VzrmI73kr4DKaONCLZpMiwDltf43QkifTultNDZEj8I+w54txIFs7jx6l/b6nLn+7WLPy/GcgzKdX9m/kgDtU/9oTmSijGyKfiPEkPp/WpfrvZCCqgZVfTuW2OWdwG8+3etGdY4yRI1v7leHZOuY+roMnoRb3j+EQT+RyKU9p1AiBZ84Fu8cSyc0zjIn2xPD5JaLIxs1qSqdBtYEXoprroeWY1ViQeWc8709RS0SPPrYYywWByPqWCaYteenp7rmnDLUIkeO6mQp75PTlFgUq+0KzE3q7JybRG4AGiEGxeyYhmdzfwbFZxllqdQb2ed+q+tKxDpb5j6/c+u7du34e2jMiU4fVPUG8AHg3cCjIpKPNMssUH2GqHT/KvDirrr7zaK7WahdrJ4ttPr3oAjL0LbtCDvvYuUGbmGIDViysj6JCou0gUMX0RD7fZGNWQZ22ZZvFy0y80oDt2PHXe3oYrT4jEn8yZGXSqqT23u8WrM6Ns/IjAhqjX9p9Qjj48a84PMzZd15I5ZiRPbhKMvMyt15M0mhFGYQg8qYDxviVYX86u8iA7IYzVs5V3lO5fSKxKl315Dr3lVPzSVN3dt4ZoJbOAnS20N6H8E+pw9Pisij6fsF4Hdjmac/APyRVOw7gL+fvr8//Sbd/2ndxZvBEGVpov17PJ7bWj6FqmVfci4PJiPFTn7GicfJOG+kVTEoqJRA1M4s9BIicM7hk9igmG6gEXPd7VRpoxLFDUlwk1IoaOwDrKpExAnBR1oHrZhOwjlnyVWTRj6ouSCX/cwb++7tW6Btf6382/ex2Hx1HIWpBToVX6F0V66Rhsg4CKxLYWYsga2Nj/V30I/k5zsEdZ6ImCt4CmQj4ot4khFHJC5BvOBUcAFzZS92TXB2nNtp188fTLPu22T7KZgTA+b0UFNEZer3Ppt2pIDUfIAbR58y6W4ey337VsI+xkvPAD8sZmPpgL+rqj8uIh8HfkRE/jzwISy1HOnv3xKRzwDXgG/btzE1pZlia2uYUh7VctwuJdDc9fLdWlwLIeDSgrVzh6QwTFaNq9BCVAtGSlqzWdlYvLOvWCwoSwjlxhacs2CzMcY+SU7p5VhuwjJDUoa8iUMILBaLvky5IafKlwFepxZUOa4lB1F6pIqYCfa2eSiNwwKmHN6Yk7I+QDSwWCzRtYJzrEPi9iY4+7l5vVcox2S0yZVNojbx+hHnXyH6k27gBwFyJhohovmIcG4RwrSyZw6J1OxcxrAjB5y0qXO5HLG419xLUyCFMOghorJICMBh+oTGCaKWQ9JyQ0a6CMFJovRDGPPcVkgckpAyNeVoBd4CqsSIqnE2nW6GG8swdW5fnxo0TdPrBWpqVT6fE7jW49qP0QSbXF9XVRZeNuz659yf1xl5qFH+um/GISmCsjxY8mf/zJ/mAz/1f/OzP/ch1utIp6lfCG2phNPNk6Ya9lEgTo3Dtns1uw9JpGF+44/Ge+LcZpJDGenK83vmkWBE//mkDq+CM2PRmAelpM5TG76c5DkqMLWI699TXMmG0qu6VrPm9fNRlS6aqbFrUoZp1ZHN+hSrjg4JZyCJCjEO1hVF36c27BSCqBdwnd+hfDYjgxrhlHVMzU85PuUY9e2pNkctT28iuBlOrvjeHrd87rO/yvd8z3fzvf/+v4MQBpuTiurW3+f6f1rCeD8J6mh82BQ1dik5XYpzeT/gzCCFk8IubPvA3mkov3jp1IJPCruhyAimJ3h+IZcLt9x0pWKwjqswB3PUfp/x3EYhy999uRr3VUgmt31bxO6aQEQC//Sf/X8cHC55xzd9A8tlg/PT75tr8/2AkyKEk5SeavGuftxPBHVmkEK9saa4hanfu6CvRwdlZo6qK7GmWKY4XOAsDXwR3Si3yUfB5T2tAZcs8zKnEMWUZkGywzO9fV0WYXyyfMvWbsMYpJRo0VLRl9ZqJZSKv1quz33OiKLkHPIpQk2Bamo5tdHr+SkVjCXHkes0nn9TxBi4odArxrocbk4jli0ygIuo2G9DsIKKRdD+3Be+wl/7gR/ilz72cf7kv/GvsVCHCKhzeDxOHU7HbZteHPuGSR36MMcxjaCwOMzrLcfw3BABqrZlYlKKl9uSwpykzfvCmUEKu8SBqQk4CXbMdYyCgxTvtfrH7KzPix6TyXuNfr/5XB/WPEcxFkmbNsbxeptir5mWzVUHX/zc9tyu8vny2ZEJMGwswKl7dZLYckzrv/WmzmO5IULFwiS8Et3K+gqNa/+jFGVARr/LNscIP/Mz/4Qf+7F/wDOveTW//Xe8IyGUsRJ0HzgNgZ0SNUs4iUgy1dY5MaBEDvsiiW3tnIMzgxS2Qb0g9ykL01ri0YRJJGpIh4MZWRTKqdiB5tgJxcLtw61pjzTSzVk5tafeE2ryHonkNiddRJTCYEaLAK6V3iD/9d739hNTSDbGyHK53NAzzMmrsqU/GcrrpbdozNSdTWQy0ikAjZunvFOUNMZI1ymf+vTneOHaS3z7H/82Ll++TIlI5t63jwi0C/bRadUi0kkQSF5P90MkOA2ncKZOH9L3jcmb22Bznd01CP3iY2zQA4aBPYWPhLfz8xzFp0kSsL3fgp9IZhPFxJJlk87s1ffoJqoSMJb78cdexbVr1wDbkD6dJ7vUFi+OrusQ7+hSkJWg0eI9FmOST1BKqh1CoGmanV6QG6cw1d8Slstlf3IxJeLlMay5B3OWAs/mJjf+YNi4Qe0IViqkaXEcCt+I7JyWTn0uLBcsD5bcOmq5fbzGUSCcCe6nbKeJD2NHrn2gRlpzHNG+HMOIkNnZ9XDvPm7PV9zpw1YQQOZdQxHZVADOVbUTYTDyvtRc/RSFSWbWWnrPr4rdAAAgAElEQVSlybBhpTh+zPXFqNy+e9cWQjQHo9zwzEiHaAggR1pWAbLDkzVmxPrnBRWj4nzegruVd0N9KXScTAdnmaJ2U7qekjpbe7IzV7qejmeNQxjqippjsmTuArPbiNobLNdihyZke7xWbt5acXx8bF6EmTrP9r7c1HJihHBSeFCKzgcJZybyUg0jDJtcj1XGXo5AL8/3v5mWf+fYxg3LP+/TsR/oKKpQOoqMEEPoU7pl8UFELZscKYYaoM7ct6NaSLIYTZl593iFJ/ZiglEDQYl95CZDCpZxKjdATWi2nxMiEs4N8SdTfLNSlzKlz1Ck36yk3JWZWue6c3KZKba85ujy9d73goH6zbLw4vq8FbnuUhRDteBiQEMyehKl0/Uw9xpt6Ge4zE0qvrEkNmCXfmIboqzv122YrAMZ4ampADOiNqbZRXqKG5kMTLMnAjxTnEK50KbkNo3jBCC1zFtTKNh0y83P5Xv5d9bM5wU9F7RVxJKTbMiBaeGGbrAa7LpxlOWxvkP6iS0RWExcQm8K3QewNV3GVH/A5nvM+mdEWpgXF997//0JLkB107ai/lvqNfLvWgG6Ld7j8OzYanKKG6xPSEqkX2/0XYFGara+XGNT+ov6+TnYpvfaJtdnc/dt9U69f6tNwj1yJ2eKU5iVw/pODt6PU+yqqo6cYeoNXb5n6t0wb3U3LOqBMR8hsNS2pmmMM0iFLbnKZu6EKDI6kpTEIYhYZGMkpWdLmztLDyGESUmp8Z4ujmNK5mCyPbVNyMUQho1nVO11Gf2YFkFoSU+MuLM8Hnl84rxBlaoSJdWnENW4pyxu5TmrkXutm9glvvTcyQz5n9us28pMUfU5mOIoIjqItTouC5nab9/gJbHo2yOjAjvbdlI4U0gBpgd3V7e3Tdo2EUKr6yWiyWHMy41fPLX5bgHVSBc7c8FJi36YtGHTl/JxVFM0DhmRNG1gm3x1A7tIUoCSytT9HCEEEufAZg6BjdGqFn+muF3XjYrVXNwU9a/L15zBFKKtCcFI/Cg4Ap9Eu6nn+78Tm69ue420hmHYbma/DaaIlMw4+I2e29MKcQqBzep1Ssp1CjhT4kPdk92TsZ+z1DYkU2/6zWfGdNkI7HAsOV5Etihj1AmKpRuLppb7hk0k/cdpjkxcto1RGbswY9I9Q13LDTvHOc1xWhvlRu8WpPLO2/XeKWSxjZvbRNSSMn8/YK3hfYb7ZZZ8v+HMIAVzlx0MXi3pCICDKJZeTCvq5MzNdnDgTOWxpCKq44UGQw6FMvZfhvy9j5GAx4knp2sxNt4jzhMV+56pDwLRqHDEEWKFiNL7S9PksoRtJGeiBh7EThFcXMIooAjJuk9S/wXS2JVjUyOIOSpYw5ReZoq6jqh8dssWITpH9IP4UessStm71Evk6yV3MBJZijZk8SKXCTGQk+CWNoo1st8lLmwiGzbu7yNiDJyg9lapFDqdIW7o1GcepsSn0qV6cMffWs1OOHPiA5SdnzYCyn+VTQpWs4kmAsTJxVVbB9YKrXpRln8t0UkY5OskITorBGS2ncRomPhQbu4cPyJnkOq5EHs49dHiLeR2TYoIMyHpgP5Ysx6fqTHdHP952Ng4Kr3mXGPsQ9aVytv6HWWWqbJMufnKU5MScdTIvESyc6JNiWjmkOQ2rmmbXmqXzmpUbk+Gpm7jPmJMCeVaOMmzZ4ZTgJrd3C4aaMLCGuIQ1TZuLnarcjOwZ223P8UuT8nP9vKhTG5LSITVjh09OZK0iEup8Cp5Mw6sfd5MsTezNq83L4K6OIpDmcGORwfF6xQCU5M5JsWocpFPUrlqrOdEsaJBJvenT6kXyHXUm7L8lO+Z2wj5+tQJg4iJUfuKRLu8DjfqZoysTvLc3O9pSDYje9Q3V2asYzFkuc3prIYzySnUiyR/r+9NPVMvqnoRmoY6aemrBTu1aYbrRvJNTu8rh8wKS7ZsZJItFnHG3KW2ee/7k5L82kPf4IG1wtJ7c8UWRbzrlWz9hhCQ4AgxIJqyT7txVKSsjS9l7Z5yp98xcSb7ihdTm25O5KijN5XzU4sDpXhVt3WK2o/eY7hvw7pzGzXft7/7PF/3aQ72pdjGWG6Wm1qnu+qeyxq+Dc4cUlBciqKTWHIdbxwRV7CPhfyUFoZqnerNjVlQBuchywNgDxtzMqEtx9hhL82wIQrDkZEfgmarxWBIQJWs15AoeMl9aVguFoTQEbsWL7bpD8TztCx5sVHudmtUAwvvWHcti8abYZKYfUNUZXHgWR8H3MITgyIh4J2YWXAWG2Kk8UNYsxxERYOdLPQIIXdako4CZhf63CKz0GjTiKFm6Z033YyqolGIGizyksYNm4fSnLvnOJKdhXeOEDoEHYKZiia7je3m3icBrQR1SebWvY2BkIy0ijIT77I1tePdtvQ3RI3SSa5/R//6omwhSp3Gj+JeMkT9TRH5rIh8OH3enq6LiPwVsQxRHxGRb9q7NaP3pgZWrGIZZKRU6JRjNeYqFO/HVouZundd18v9uapy4ZnjTUcMwaIYp7/lp6fM0bIehRAIId8bchd0Xb4XQJXVatVTRVV7dhVaWg9tt6ILKxBDPgfLJY33RT+UqIG2W1lEpTKAiiouczM6OHONZeIx95TbkBHbNso8J0oY+z58L49zp55TZTx+MuYmplj2+r6I0IVuT7b85LBdN+DmuPzh+b5s0Z8T2FWfFHHdL9iHU8gZom6LyAL4JyLyf6Z7/6Gq/mhV/vcAb0mf3wb8QPq7Jwzy9+Tdiq2fK5nLjYw+is2OmGJsIw6hFGf6GIvvJhZdjBHUzJJjtDTq/eIU0JAWumZX4mGSQww4EULXDbJwCvL6/PoOq9ARnaBdhxfh6tUrHK0MAcQuEFQ4WB7SdYFLj1wihA7tWiiUjjFzM6oENc5FMAvsUsYcqHepkknHjElk2tj8M1Aq++pxr8uNEE3SpfisvC0Qc64jhDCKAak6dniaateUR+r9gUjOxLUPjHQpZ/QYsoSdSEFtFqYyRM3BHwT+p/TcPxORR0XkGVX98rb3DPH8HKqbBip9uQn5rVwQdejyclGJSP93iq3N7NqAzRPlLJSSpbxux6Zm5OTEQehSklhJQVsjdeYma0NrwVhiZLlo8AqNc3hxHLetrbdorTg8PODo6A6XH7lCVPAx0gXh4MIjvHTzBqvVGhFwjadbW9+8s4zOEiOeYdOpDcrQ7xjxyXS66zoyByqqqIYePbuC6tdydO07IsW415t1NM4mOA+iASkJkI5PIfLz2fMzX3MpsU5GFFN6BBElu8KfFDbXXflbeu5mCvo+F/0dIa0HxAGUEs69eFeeKkOUqn4w3foLSUT4fhE5SNf6DFEJyuxRJ4IN+WnEfm4ihHLT5oWS8xnAOD9BDb2sqjoSJ5Dx+flYMTaY5GqMLJocJt4ltn2zPwo0zrPwDQfLJY8//jiHzYILiwMeWR5ysFyYC7YIh4eHiAhPP/00zaLhta99LY9deZSnnnqar/6aN/HEq5/k6tVHOTg4ZLlcsFgs7LnFgeknLl3l8ctXWLiGReIUMrVqxJBQvbn7v5o+sCnPFx2rOa1aCTglbmgWbxgUiwpoEQkriyDle7J+IY95qYic4ha0Eiu3wcbGZXNtjb5vqXcQxca/H5Y4cFLYS9GoRrrfLpb/4X8TkW8A3gd8BVgCPwj8WeA/3/fFIvJdwHdt3smKu1JMmFZ05YHXOL0oJ945KSPne6nUiMVLOp8BexapwUaLKEZiA41ERDwrjSnfgyEZLxaGLQo0osRkcHPr5jVcDDgWPNE6ri9ir1BRVZ559VN80zu/iTt37vDHv/M7eeH6XZ67dsydVeSjv/Qr3Lz7HM9+/Bc4eu46XbzFenkAXrgoAV3fZu3AO2XhGho8su6IXmgiBBdx4kwn0VioM9qOBuNcDDkqXQRNXp39GIbCBDqJQCVlbBC8GrcU1MY1FPddtCjN0g+y9qKUXVI0Kg1C56BRS66brSd94upixTnWiChqqJR/2UJ0u/J0G0EaXVfMEK5adiMkJUVZZeNkYbQeK2SzMxHUTqRX0v3N4L1TcKLTB1W9ISIfAN6jqn85XV6JyP8I/Afpd58hKkGZPaqs6wcxZILUu353O2aowsOT1xqxMO6viQtuN8rdqk3eOw4OFrQxsHDC0dHKxIwIop6FeP7Np7+B77v2EV5KrlAAIbT8/t//e3n+2Wf5Xd/yu2g7z0c/+Xne8OY384/+r/+XF68/y8+0d3jk+U/yiwcrBDi8fJHVS8rxgaO9e4wTYYGw0gAOljiz/XPCIgjR2zLwgDqPxoh4QaM3hCHpFEV2bx5lONGI0B/VJrzQU88om8rHEWVOtifiHU6V1hJ10vSYJIkblcjwoNfB/ab2Z5F7OG2GqE+IyDPpmmAZpz+WHnk/8CfE4F3AS7v0CftCLR70OoCScyjKln9rebMuv+19Y25izFL2i/lwydd/9Zv5fVe+mjfHC/hgVooxZal65jVP8vXf8Bbe9Kav4smnnsA5sbT1ndJI4A80j/OH3/gNHKSgK4Kx8jdu3OBVj13lhRefJ7SBj//yJ3j/+9/PP/g/foIbL13n2nMv8DVf+0b+9Te9icNOuNuuuHXtOpdvrfnx43fxHv8Mb3vr1/Pk008gaintnlheYLH0POkPuHrxAgeXD2iWnguLBiewaBp8VBqMwwEsUW6CcsuN5kNhIc7Us6J4FBcst4Z4jxTc30hh2OsJXJ+SDmDhfZ9E/lAdh92wYMVJf2JRIpQ6iG3f5p4DOZ3ycU58KvuwVUTQzXL9GJzgRGKy6vuMWO4lQ9RPi8iT2Br5MPBvpfL/EPhW4DPAXeA7T9KgOfk0/56T8+oycwqubTJvDWX5iFFSEgubF5/3nsViQVyvCCHytuZJHn/0kJ978SNoGNjZR65cJoSOVz16hS/ctshLzWJBWK14z4Wn+e43v5O//ZF/yjokd9okLx8cXORHf/TvIc7z/HMv8Ff/6n/Ls9du8osf/QSK4/bNL+O6I/6fr3yBLx7dZi3KrXbFXQff3X6QVSN89RvfylOXr/LFL30F3zj+6GNfy985+iSvuwtfe+k1/N1bn+L1X/U6Ll+5zIvXr/P8rz/P063yZRS3XHC0smNU1JKxrNfr0Tg551gsFrTHq55LcMDVxYJGldsoR9iJS85m0UeaTqxFre/JR6ldCLzh8BFuS+DG+giNpmAUFaLY+E/piqY4yQxRBzuSfWBq/YyusYPYTESzGrXzHva0KL2Nwr36PGTY5/ThI8A7Jq5/80x5Bf7UvTdtVOfk96nNPKfcmuMUTsxq6qDN77MpOWHZLFHvudOu+Zm7X0RCWthAJOKccOfObUII3Lx5p1esBQ18or3Ff/KRn+Q7Lr+FoN2QQ7LrODxc8ta3fgMvXLvGx37po3RxzWp9hDSHLC8siHeFu80RbbvmrnQsjgNeAI18UY7wy0MufPRTrNuALmAV1vzgjV/kiadexWeOnuP2zS9xs11x+1d/lcaB9wtc5/lL7/jtvO+DP8uvH3haUSKCd7BarUzBl4Yjj0GMsT+6jaosXcNjvuGx5QFfbDvW4cg2sLg+kpMrEIJP8SDy6ZCAuZQD1+OKO+2apmkIIefCtKhUIikuxZZpLBWcU1zl5DRPrKH8vby3S+m4C1S1N4Y7K3CmfB+AcSJNyZM93czaE9KuTYsSU6zdlMa5riuXcbme5JkZEbpom2W96vAO1m3Hf3H8CT4UbtIlRjW/8fnnb/DS7RVf+soL3Lp5Cw3QrgPOHfKFcMzrFld562vfwCpCwKE4nLNEJ3/sO/4Ezz33Am9729t435/5Xt75W9/JH/72P86f/Lf/PV73m9/CN37jN/FmucxFERYXl8RGOVgseVVzkUNZ8sirn8AdemIreBwHIhwdHbFolvy6tNjVhhgd7brlKBzz/PU7yKHnWw4v4glcFqXRZBgVgyWA9U2fCFaSUrDPp0Hg+fVdfuHW87ywvkmjSrtesxBHk8UM53DRRJQFyoEIy5SxW4E2RgLKi+2aI4GbXUsLKQiM5XbQaLEn+mSy6kA9qKdPDhwFjdL/NkvW7ZtwTtdRr6m6zDYOZer+qKxsliujZPX3dPiM6lbXf4Y9Uyah3Q/OHFKYh5Nj0l2UYUq23aPSkXyZWdfbd+7ya5//PHe7NXfCmpgUjyEpw7783Av8+pdf4LnnbxBiIGDrF1VwDf+Ya/yhz/8Emjj1xWKB95626/gv/9L389hjj/KJT3yc69evcfXqRZ569IB3feMbedXVx/gtb3wLf6x7miDCwZVHeOrS4/z+5ev5vktfz5Pqef2jT/EtF99I8NC4hnc0T7LolKeD593uMZSAc4p6x3J5yAU54PVXHiHEjscOHyVq4C0HV+k09FxSOWa9iFVq/UOHojztDzlURxsDoe04Pj5GoY8O7bzjcLHk8sEFM14q6ssh6RBM+YlxVyMbEwbOLR8/x5g/4/B920+dNtfGvuviXmX6zCVMqdt3vT+bWW8L6Xbi9pwF7aeIqCO50c4ZhCgMB4QZSmy4HaZYtJOIDr5kF6koicLCCRdcQxTleN3awi/a5QtFncbI4WJpu9/D5eA4dA1rt+JmMI39wjc4t+Dxp66yWHoWywOuXH2UDkcIHhZXEFlw88VfY3XzBdbru3RHa1h4rhxe4vjObS4dBa5rS3Ow5HL0PBeOEODNjzzO5259hd918TXc7AIf1us8/uqnWLUt7fGKF1+8zv/+W7+FP/3BD8CFQz5z6yUuHR5yMwRWXZuGfrAhyNxBI9moC640jgsiPLm8xIu0/PrRbTuilGROpWaCjSZOQTxrjQRV1iGMKaPzheFVkuFF+mCvajLdmKVnh53ECWAfXVb5vqnrZT1TIvBcnXOBWzNYFrIcvHiYE8cmMtk3xPuZRApTgzsMRIkAtiOFXQM+JR/OIYqapRrpKqKyFFj4BlFYdy2KENym8kuTwu6wWeIEOu24pA2HzuMJvIhFJPbNAu8aHnn0QtJbNKgI0hzSHB4iNPjYcXznDrfWt2m7joUqTRtZS8QrXOKQVoJRaY10RFoHF1q1vHQNLGND6yG0rbH/Xli3ymsXwp3WcxwiL9GxVCWIIwjGLfnBQQwRpAssxJE3/MXG8TWLR/hSd5sjImvv0WDiVslleXFcXC4I65aVGLfRpRiX/SaScRDdwMA3zomLc/MtMjZ9n9vI29j+0bsKYnMapLDtOdiNFJJaJsHgG2MjPX54X6Rw5rwkYYbVl92GHA8a8gTUiicRiDHQ0tEkhXog8zS5rB1nCop468/ywiHu7hHHjUIXeKQ5oO3u4jTFZSBy/dpLXL16ha7tWBwccOfubWR1bHkV1dEd3aSJHb5d0eFZx0jjPK1EFkuhDcpR7Fhp4KI6fBe560FCZBk9xy7iWqVzQquKC4KThi+FyDLAHQcu2RkM1Hr4qzokiNVkFg6wInJLAleaQ46kRaJ5kIYYe1fuvBliF7h4cMh6dXfECPYjJ1Zt1jVMoe2prNn9vG0RBbYRxV2nU8DGxnu5Ye7t99KuM8cp4DYxeT7SsWStm1zBPkcxJULZhpmrdo3fo9qLEfl+/mjo8M5xwS+4sz4G14za5RjnWRQRFk0DIdo5vGuQELmm694mwHuPepf8KRziHW203SGusaQxXYsjoq2x3AcHBxwfH/dOXx4ZvChH7bFgKCLCumuRKhdled5vFF1GI69uyKwkmrJIrVtEzQJViFxwngu+4cZ6BU2Dx3MU1kOI/Gjj6SI8sliwFuXOemU5L4oxr+chynD6Uep3prJb5Tp2zfHGiUJR90a9rjCzn6napaPKsjVznGu5xsv3nga2PfuK5RRKFqv/m+7dD5+3uu57gWx/b/b7yXQZ6MCyFRW7sM3m0cU7112Hc2YSHGNkKZ4uBqJLJ99dhwbh4sWLtOs1lw+voCnQyqptQeHCxQvcvXUbRI0Kr46Tp2HyYWg8gin0cnxINCmmYtoIzvUKvWxG3COIpP3uQ1xk0ELrHpMbOuZr4EVwzuP9gqCwdJ5W89FsEXY+O2qJcLdr6SQpEDGHp7kNLjLmv6JVOKtUPslcT+kj9uEmaoiq981u4F7gNOv87CEFhgAmc3DaiYJpGXT8/vS/CAMpNP5Vk8xbtrWP3pSO49bBTJSD6szRzpgJVoVOlUDHXTpaEXzp8hvh6O4x3gm3bt02jbyaeqlpGm61KzRGUoI7jAAP6Eizhl7tCDc3QdIYuJ56xSFNHvkITEZla7FpGMeEQJz0Tk1RjZO42CwRTScJkm0PdGPDZ/FETD4biRij0XPeEEAavHI+tykUJ9eEDIZHcwZIs+KISH8asmu91e2ZVFwWr7A9UNXzMtoynDmksE1pk/UKuzBfHdjzpJC3bP+WESIYl8uTpf1eL6wgy8mXEkUMbYvVAmmk2LwJQoiEAJJiPSiKOM/xcZdY9aFhRrmH/ofjFSJiMRcKzisSbbMGtdgNkOJG0te3keGpPBEoG5hFqMQCByy79o2w5k635hG3IBnd2aYnezs6YtfZ896ziJFOAl3SO0zOYd741VycCqRQkUz1a084KSXeZ23WOrTTIoSomwrHXXDmkAJsapLnOrWPtre/dsKB2YftmjriPIkia1ROYBGzH1tZrw6Uqd8JxhHY+3LAmIL1nXq/DrEaFct5mXiLFGxWzEJQizOdmppP9K23SygsGnN/RAFxvKStXQiR6KQ34XapD3lneu9REbp1u9eY1TB3YrCLaxxdKzjVrdzojrbELZT9NCz9SZ/pEc8pMOeZQQqRHGZtKgnqAFqe95Opejn4Qy5FVzDwOW/iRt2qFluwPysQLM0b1CNqCKpMqVY8pxMsaD5K06FdNQVQyVp7aM11yNRTZXvzxiuotWoYZPpcb7pp8n+JEDfXRm3s0osSpYi0Y1OU+h7jYHLzpJ8GvzAHp67rUJ98RkSIIYBP8TMVYhdYeI85zG7hA2KOvyADh1KJGluRg0zXPHB0g0GUFNf7OtL/Uwi4BJdeVMZOHI7bZeNJmYkSle0Qth2zln2u/57GNuPMIIWXC6ZkvFhsbruxH4u3C05zhNovXN08PpvaABvPztX5gGDELRRtk/T9eGUWjDmqsEuK0t7wKMYURk5NA6/jhCZ24jT2gowxGKekYzS88/hQZAMZbu1bQtabFe1dxfgxBTY431JluvmyXaHZd83taeb+zCCFYTIjG3kaKso6/dzpIVvWpRr72H8l9AvuHvfXNnEINyj9YBxaLj9bfx8hirl3nkBJdZpFNCky9e0aI4wAlkCXfKTpetEjJI4tJLFD0rXMLakqFy9dIoRA23V0KZDuvm3PCHfgEHf17P4r9gppae939Hqr6u8+IsVIcb/n3J4ZpJDhBG0/Zf0TkzDxvnz0tXG9VLadoJ09GzrzvvF7d3t0lufzg/w4LyPfb+gVnjN177o/KoulBAw5NFFWkibRrkSAt27dGpBO8a5SlJqD8hj0QcI2Xdfm9d4iY74+pBcj8m+odGZz79wTeZRwhpBC7ujQ4UFXUpiSqmw9/7UwbhODo8M7hsJZMztiQgs/i+L5vKl1XHayDRivM2Zts8xaBUAdVRH7d4tWCKRe8DHJ8xNYdDw+w+bZGdrrpMqvHRt++/1yQadTGLINj24y08XJidlaTIcmKU9upsL4ZYopSX8y9DnrZop27TEcm6cE01wcnGJ8i+drMWJOl3A/4Ax5SZryZeic9h8tvmc4zSDUS7RUHY1h85y4vFe3Zdd7Rve2UbPEPmsSZeNelGBnc8avOIH8sw/1PT2Mx1GrltWrIH/PuTFPMvtT+plNGCv/Ttvv8qkHy4/sB6fpxxlCCgZTnag1wDJwmHs9f5oyGU6Lgeee2sgzMfns8K9oyKxysdy8O1lo9t/o+22mlwdOq+tQ1XF8jopyz43F3AnGrnIq0meALjmG02zObX3ed25OM4d7IwWxMO8fEpEfT7/fKCIfFMsE9XdEZJmuH6Tfn0n333CiFjE9GDoa5JqN32gria6AlEEmihRvRb3b4MFRyvE7yk3tGD7jo7BpKCd+L4Sz54J/uWBOj5JhH4S3dfEncYMKIZTv3gXbEsuUayi3wyXhqdT97As1ErmXGI6nQUgn4RS+B/jl4vdfBL5fVd8MXAfem66/F7iern9/KncimOQWCsZsHy1zX37HeO6Ui+/rhtmMXlxO+L4KodLqc9+25vdN9fescANzcK9ysxT/ttU/eqYe2y2GSOW9UiE4V/cuqJHI1Lv33ewPjFMQkdcBvxf4H9JvAb4ZyCnjfhiL6AyWIeqH0/cfBX6nnGBnTWNFKT7lte01zV/fVyqV6u+9Qt0Hm/DdEzd+Zs5efxtk46J7oTrT83DWoNRBbcJuXVH6VYsFG7XUn836t4+SMrUWJ1f/3ttnuk0nhX05hf8a+DMM5yePAzdUNR8Sl1mg+gxR6f5LqfxW6GPO6RQWlCSrlZt0O5XUmeuSjrmy+DE34CaiDNZnUzHxxnEkN69P93OsNR6xiRNs/WD9VrGUwmiT92fvYpmVJvUP6bivPwjZsdgUrd4pxWeznydFVtv6flIY2mMOyzp3dFCsrXEsUN14ZtTH/vsmQijtWkYc7VQ70XQqkmZP8ifPqW6MbX30mK9tjtnLhBRE5PcBz6nqPz/1W6br/S4R+XkR+flRg9xm6O1dbDHUiyrL4cM5/vQmMe+9re2cmNldrNu9snaTfapZyi31T/ksTOpp9lFUneS0gnkR5UGCstnHub5NlSs/e73vHhFYZGy1WcLD1u/AfnYKvx34AyLyrcAhcAX4b4BHRaRJ3ECZBSpniPqiiDTAVeDFulKdyRA16dUVhyPCDao8qaRKmDLpG+3WZgLSWRazKjeVEqx/zQQM9xMVrzMUzwXp6Ns70adeeTq8QyYMeHrDnrTIBTvj1t7+Im/0gVqO+j5qT7YN2L5QY8FRGDcyUVcFJWezS9E4BfUR5mQZmVYkDn4qOxuYb0MAAAZaSURBVDilGSQ8pcuZGqepsc3iotgLmD1Gm6kjahzrLLbNHw6VgkDsObw7OQVVfZ+qvk5V3wB8G/DTqvpHgQ8AfyQV+w7g76fv70+/Sfd/Wu+jJqsMb11T1F2Ktw0OJGn4a9jW3Ptxdt+3N1PWXJ9ultmnnm3t7JWShfixbZOfNDDIw6Jsee7KU5qp+Z7kEGWsrC0/o3fMXJ/iKpTtYmPd9v7fGeAMargXi8Y/C/yIiPx54EPA30jX/wbwt0TkM8A1DJHsDaXlIiREWqC/0mJtY2ImfkvyJyA6ImGrg4lZTrpeRqwpXcTcfYe2DvvYi/SGNQO4/rnx5CdcLEMwEUQmPeWiRHJS1J66a5ZLrS7tG5Oejxnp5KoF5x3LxdJCrGvJuhbp4BRKxdekFWgFJZXf5bxTPzf1d1RmD44jQ5kQZhsxKPUJ9fM15HkbI5fS/F17PVj2igSIkrmGUrYfxrV0yhremxzGnMN8X/K4DA21Y864kR+ib98oNk8Ki1cSms0uTsKZi9GoxRja11GUO8pFehKqJspOpJBeCyOX5wxx87ipqMprYcVebbh9z8KnkQKGqCqxRyY3dS9MjOtNC8en2IhRy2VZIwXYDHyXN1LppbmJgE+CFPaBkyCFsvzWMnMiwd5IYQyTz80ghSnbmink3CshVSnDtmcX/CmnQFUdialWJvZu1yJC0PgKjdFI8urqr9yfhZZZRoUN//Qsn9rgFtNYYuQdC3PYjiarWv1blJwTcvT2npoT8WYdZR8zhzPNQeWUbGjWgs+9UWa+M6KYm325X7M1hpEtxyne8PITvu2IZMxPzomvae1sNH28Dkpfman3qG7qyHbB2TNz3hgkqT73DlM+9VlKOS21q2u8n8dtA1Qu5bOLrmRWq3t7teX+jve9wrb+vBLhvvdnpqLTngKdOU7hpFjtvryzHLx72AcDFR5+T1GMfcWJcRvT/xOUugYd3Z9AsyL5SGb83A7O6DTtvt9wmvVx2jbfD6XyFPhiRoqkdvSzvI+ouW0cqsdPPF5nRKdwC/jkw27HfYYngBcediPuI5z35+zDrj79JlV9clclZ4VT+OQ+CpBXEojIz/9G6tN5f84+3K8+nTmdwjmcwzk8XDhHCudwDucwgrOCFH7wYTfgAcBvtD6d9+fsw33p05lQNJ7DOZzD2YGzwimcwzmcwxmBh44UROQ9IvJJsfBtf+5ht2cfEJEfEpHnRORjxbVXichPisin09/H0nURkb+S+vcREfmmh9fyaRCR14vIB0Tk4yLySyLyPen6K7lPhyLysyLyi6lP35euv1EeUBjBlwPkZQiL+FCRgphh918Dfg/wdcC3i8jXPcw27Ql/E3hPde3PAT+lqm8Bfir9BuvbW9Lnu4AfeJnaeBLogO9V1a8D3gX8qTQPr+Q+rYBvVtVvBN4OvEdE3sUDDCP4MsGDD4tYB5l4OT/Au4GfKH6/D3jfw2zTCdr+BuBjxe9PAs+k789gthcA/x3w7VPlzuoHc4P/3b9R+gRcBH4B+G2YcU+TrvfrD/gJ4N3pe5PKycNue9WP12HI+ZuBH8dsF+97fx62+NCHbktQhnV7pcHTqvrl9P0rwNPp+yuqj4nNfAfwQV7hfUqs9oeB54CfBH6F+xxG8GWGBx4WEc6ATuE3Iqih51fcsY6IXAb+HvCnVfVmee+V2CdVDar6dozCvhP4zQ+5SacGeUBhEafgYSOFHLotQxnW7ZUGz4rIMwDp73Pp+iuijyKywBDC/6yqP5Yuv6L7lEFVb2CRwt5NCiOYbk2FEUS2hBF8iJDDIn4O+BFMhOjDIqYy96U/Dxsp/BzwlqRBXWJRmt7/kNt0WijD0H0H4/B0fyJp7N8FvFSw5GcCxNzy/gbwy6r6XxW3Xsl9elJEHk3fL2A6kl/mIYURvFfQlzMs4hlQnnwr8ClM3vuPH3Z79mzz3wa+DLSYHPdeTF77KeDTwD8GXpXKCnbC8ivAR4Hf8rDbP9Gf34GJBh8BPpw+3/oK79PbsDCBHwE+Bvyn6fpXAz8LfAb4X4GDdP0w/f5Muv/VD7sPW/r2rwA//qD6c27ReA7ncA4jeNjiwzmcwzmcMThHCudwDucwgnOkcA7ncA4jOEcK53AO5zCCc6RwDudwDiM4RwrncA7nMIJzpHAO53AOIzhHCudwDucwgv8f94Lh3OdWC0YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_test = 50\n",
    "print( data['boxes'][idx_test] )\n",
    "plt.imshow(data['image_data'][idx_test])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear logs\n",
    "shutil.rmtree(os.getcwd()+'/.logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 1st pass: Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1st pass: Transfer learning\n",
    "number_true_epochs = 2\n",
    "# Create the model\n",
    "model_body, model = create_model(data['anchors'], data['class_names'], load_pretrained=True, freeze_body=True)\n",
    "\n",
    "# Configure the training\n",
    "model.compile(\n",
    "        optimizer='adam', loss={\n",
    "            'yolo_loss': lambda y_true, y_pred: y_pred\n",
    "        })  # This is a hack to use the custom loss function in the last layer.\n",
    "\n",
    "logging =  TensorBoard(log_dir='.logs', histogram_freq=1, batch_size=32, write_graph=True,\n",
    "                                       write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "# run: $ tensorboard --logdir=.logs\n",
    "validation_split = 0.05 # Do we really wanna do validation on each chunk?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      True epoch: 1/2\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 31s 10ms/step - loss: 406.6783 - acc: 0.0000e+00 - val_loss: 199.6565 - val_acc: 0.0000e+00\n",
      "      True epoch: 1/2\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 29s 10ms/step - loss: 160.7024 - acc: 0.0000e+00 - val_loss: 162.4567 - val_acc: 0.0000e+00\n",
      "      True epoch: 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-5ee6ce132855>\", line 12, in <module>\n",
      "    np.zeros(len(data['image_data'])),\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\", line 235, in __getitem__\n",
      "    pickle_kwargs=self.pickle_kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/numpy/lib/format.py\", line 683, in read_array\n",
      "    data = _read_bytes(fp, read_size, \"array data\")\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/numpy/lib/format.py\", line 822, in _read_bytes\n",
      "    r = fp.read(size - len(data))\n",
      "  File \"/usr/lib/python3.5/zipfile.py\", line 844, in read\n",
      "    data = self._read1(n)\n",
      "  File \"/usr/lib/python3.5/zipfile.py\", line 934, in _read1\n",
      "    self._update_crc(data)\n",
      "  File \"/usr/lib/python3.5/zipfile.py\", line 859, in _update_crc\n",
      "    self._running_crc = crc32(newdata, self._running_crc)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.5/inspect.py\", line 669, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/usr/lib/python3.5/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'list' object to str implicitly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2962\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2963\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2964\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5ee6ce132855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         model.fit([data['image_data'], data['boxes'], data['detectors_mask'], data['matching_true_boxes']],\n\u001b[0;32m---> 12\u001b[0;31m                   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    234\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_crc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/zipfile.py\u001b[0m in \u001b[0;36m_update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;31m# Check the CRC if we're at the end of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2962\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2963\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2964\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   1862\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   1864\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1866\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1868\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1373\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1281\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m             )\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m             \u001b[0mformatted_exceptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m             \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 'list' object to str implicitly"
     ]
    }
   ],
   "source": [
    "### 1st pass: Transfer learning\n",
    "for true_epoch in range(1,number_true_epochs+1):\n",
    "    random.shuffle(chunks_files_names) # Shuffle the order of data-chunks: Try to avoid biases\n",
    "    for chunk_file in chunks_files_names:\n",
    "        print('      True epoch: '+str(true_epoch)+'/'+str(number_true_epochs))\n",
    "        # Load the data\n",
    "        data = np.load(training_chunks_path+'/'+chunk_file)\n",
    "#         print(\"      Loading \"+chunk_file+' with ',data['image_data'].shape[0],\" data samples\")\n",
    "        \n",
    "        # train the model\n",
    "        model.fit([data['image_data'], data['boxes'], data['detectors_mask'], data['matching_true_boxes']],\n",
    "                  np.zeros(len(data['image_data'])),\n",
    "                  validation_split=validation_split,\n",
    "                  batch_size=32,\n",
    "                  epochs=1,\n",
    "                  callbacks=[logging])\n",
    "    \n",
    "model.save_weights('trained_stage_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('trained_stage_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: 2nd pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2nd pass: Train all the model\n",
    "number_true_epochs = 15\n",
    "# Create the model\n",
    "model_body, model = create_model(anchors, class_names, load_pretrained=False, freeze_body=False)\n",
    "model.load_weights('trained_stage_1.h5')\n",
    "\n",
    "# Configure the training\n",
    "checkpoint = ModelCheckpoint(\"trained_stage_2_best.h5\", monitor='val_loss',\n",
    "                                     save_weights_only=True, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
    "\n",
    "logging =  TensorBoard(log_dir='.logs', histogram_freq=1, batch_size=32, write_graph=True,\n",
    "                                       write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "# run: $ tensorboard --logdir=.logs\n",
    "\n",
    "optimizer = optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(\n",
    "        optimizer = optimizer, loss={\n",
    "            'yolo_loss': lambda y_true, y_pred: y_pred\n",
    "        })  # This is a hack to use the custom loss function in the last layer.\n",
    "\n",
    "validation_split = 0.05 # Do we really wanna do validation on each chunk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      True epoch: 1/15\n",
      "      Loading training_chunk_7.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "1248/3040 [===========>..................] - ETA: 45s - loss: 82.0861      True epoch: 1/15\n",
      "      Loading training_chunk_14.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 62.7267 - val_loss: 68.6462\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_2.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 59.0810 - val_loss: 57.8127\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_5.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 69s 23ms/step - loss: 56.0939 - val_loss: 49.3359\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_12.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 69s 23ms/step - loss: 56.5885 - val_loss: 51.6760\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_13.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 52.7499 - val_loss: 55.3222\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_4.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 23ms/step - loss: 53.0567 - val_loss: 46.0104\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_6.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 69s 23ms/step - loss: 51.5483 - val_loss: 53.9597\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_9.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 49.6865 - val_loss: 49.0002\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_16.npz with  2000  data samples\n",
      "Train on 1900 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1900/1900 [==============================] - 44s 23ms/step - loss: 48.1780 - val_loss: 47.5854\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_3.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 47.7632 - val_loss: 52.1017\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_10.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 48.0119 - val_loss: 48.1687\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_15.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 48.5418 - val_loss: 46.1526\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_1.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 49.8069 - val_loss: 50.7998\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_8.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 46.0465 - val_loss: 49.4333\n",
      "      True epoch: 1/15\n",
      "      Loading training_chunk_11.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 46.4290 - val_loss: 49.3243\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_14.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 43.1056 - val_loss: 50.8799\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_6.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 67s 22ms/step - loss: 40.2897 - val_loss: 51.7514\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_15.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 38.6250 - val_loss: 43.5483\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_10.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 37.9400 - val_loss: 44.8079\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_16.npz with  2000  data samples\n",
      "Train on 1900 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1900/1900 [==============================] - 42s 22ms/step - loss: 38.3090 - val_loss: 47.0907\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_11.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 69s 23ms/step - loss: 37.6138 - val_loss: 49.9742\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_13.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 41.3512 - val_loss: 49.7722\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_5.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 41.7695 - val_loss: 43.0042\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_9.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 39.4003 - val_loss: 45.7204\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_12.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 42.7619 - val_loss: 41.2927\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_7.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 43.2415 - val_loss: 47.0198\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_3.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 67s 22ms/step - loss: 39.1146 - val_loss: 48.9086\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_2.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 69s 23ms/step - loss: 42.4420 - val_loss: 44.6091\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_8.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 38.6148 - val_loss: 47.1735\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_1.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 40.3495 - val_loss: 47.7087\n",
      "      True epoch: 2/15\n",
      "      Loading training_chunk_4.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 41.5234 - val_loss: 41.9690\n",
      "      True epoch: 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3040/3040 [==============================] - 68s 22ms/step - loss: 19.2141 - val_loss: 47.7597\n",
      "      True epoch: 9/15\n",
      "      Loading training_chunk_1.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 69s 23ms/step - loss: 19.6406 - val_loss: 54.5375\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_5.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 69s 23ms/step - loss: 16.6460 - val_loss: 50.5852\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_13.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.6518 - val_loss: 56.1213\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_10.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 17.3116 - val_loss: 52.1328\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_4.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.3119 - val_loss: 48.4740\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_12.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 17.9001 - val_loss: 49.0506\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_2.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 17.7129 - val_loss: 55.6481\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_15.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 17.7405 - val_loss: 45.8333\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_9.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 17.6099 - val_loss: 50.9208\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_8.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 69s 23ms/step - loss: 16.7080 - val_loss: 55.6621\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_11.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 70s 23ms/step - loss: 17.1000 - val_loss: 62.6148\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_3.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 17.8872 - val_loss: 59.9586\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_16.npz with  2000  data samples\n",
      "Train on 1900 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1900/1900 [==============================] - 43s 22ms/step - loss: 16.9437 - val_loss: 58.1278\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_7.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 18.4345 - val_loss: 52.2548\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_1.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 18.4719 - val_loss: 56.8965\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_6.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 18.3751 - val_loss: 59.1966\n",
      "      True epoch: 10/15\n",
      "      Loading training_chunk_14.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 18.1785 - val_loss: 54.6688\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_13.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.3425 - val_loss: 55.3774\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_5.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.0023 - val_loss: 50.4433\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_4.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.4519 - val_loss: 48.3139\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_9.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.3898 - val_loss: 58.1426\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_15.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.3609 - val_loss: 46.4246\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_1.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.0606 - val_loss: 58.1314\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_16.npz with  2000  data samples\n",
      "Train on 1900 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "1900/1900 [==============================] - 42s 22ms/step - loss: 15.4691 - val_loss: 50.6483\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_12.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 17.4931 - val_loss: 52.3115\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_2.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 17.3093 - val_loss: 55.0041\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_6.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.7032 - val_loss: 61.1682\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_8.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.1337 - val_loss: 56.1654\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_11.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.0880 - val_loss: 60.3671\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_3.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "3040/3040 [==============================] - 68s 22ms/step - loss: 16.8788 - val_loss: 58.1663\n",
      "      True epoch: 11/15\n",
      "      Loading training_chunk_10.npz with  3200  data samples\n",
      "Train on 3040 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "1216/3040 [===========>..................] - ETA: 40s - loss: 17.4940"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for true_epoch in range(1,number_true_epochs+1):\n",
    "    random.shuffle(chunks_files_names) # Shuffle the order of data-chunks: Try to avoid biases\n",
    "    for chunk_file in chunks_files_names:\n",
    "        print('      True epoch: '+str(true_epoch)+'/'+str(number_true_epochs))\n",
    "        # Load the data\n",
    "        data = np.load(training_chunks_path+'/'+chunk_file)\n",
    "        print(\"      Loading \"+chunk_file+' with ',data['image_data'].shape[0],\" data samples\")\n",
    "        \n",
    "        # train the model\n",
    "        model.fit([data['image_data'], data['boxes'], data['detectors_mask'], data['matching_true_boxes']],\n",
    "                  np.zeros(len(data['image_data'])),\n",
    "                  validation_split=validation_split,\n",
    "                  batch_size=32,\n",
    "                  epochs=1,\n",
    "                  callbacks=[logging, checkpoint, early_stopping])\n",
    "        \n",
    "model.save_weights('trained_stage_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('trained_stage_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 B - Training with Data Chunks\n",
    "\n",
    "Train with the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear logs\n",
    "shutil.rmtree(os.getcwd()+'/.logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 1st pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 189/1485 [==>...........................] - ETA: 27:40 - loss: 261.3029"
     ]
    }
   ],
   "source": [
    "### 1st pass: Transfer learning\n",
    "number_epochs = 2\n",
    "batch_size = 32\n",
    "# validation_split = 0.05\n",
    "\n",
    "# Create the model\n",
    "model_body, model = create_model(anchors, class_names, load_pretrained=True, freeze_body=True)\n",
    "\n",
    "# Configure the training\n",
    "optimizer = optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(\n",
    "        optimizer = optimizer, loss={\n",
    "            'yolo_loss': lambda y_true, y_pred: y_pred\n",
    "        })  # This is a hack to use the custom loss function in the last layer.\n",
    "\n",
    "# This is a hack to use the custom loss function in the last layer.\n",
    "\n",
    "logging =  TensorBoard(log_dir='.logs', histogram_freq=1, batch_size=32, write_graph=True,\n",
    "                                       write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "# run: $ tensorboard --logdir=.logs\n",
    "\n",
    "# train the model\n",
    "model.fit_generator(get_batch(training_set_files, batch_size),\n",
    "            steps_per_epoch=len(training_set_files) // batch_size,\n",
    "             epochs = number_epochs, validation_data = get_batch(val_set_files, batch_size),\n",
    "            validation_steps=len(val_set_files) // batch_size, callbacks= [logging])\n",
    "    \n",
    "model.save_weights('trained_stage_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('trained_stage_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: 2nd pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2nd pass: Train all the model\n",
    "number_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Create the model\n",
    "model_body, model = create_model(anchors, class_names, load_pretrained=False, freeze_body=False)\n",
    "model.load_weights('trained_stage_1.h5')\n",
    "\n",
    "# Configure the training\n",
    "checkpoint = ModelCheckpoint(\"trained_stage_2_best.h5\", monitor='val_loss',\n",
    "                                     save_weights_only=True, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
    "\n",
    "logging =  TensorBoard(log_dir='.logs', histogram_freq=1, batch_size=32, write_graph=True,\n",
    "                                       write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "# run: $ tensorboard --logdir=.logs\n",
    "\n",
    "optimizer = optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=1e-08, amsgrad=False)\n",
    "\n",
    "model.compile(\n",
    "        optimizer = optimizer, loss={\n",
    "            'yolo_loss': lambda y_true, y_pred: y_pred\n",
    "        })  # This is a hack to use the custom loss function in the last layer.\n",
    "\n",
    "\n",
    "# train the model\n",
    "model.fit_generator(get_batch(training_set_files, batch_size),\n",
    "            steps_per_epoch=len(training_set_files) // batch_size,\n",
    "             epochs = number_epochs, validation_data = get_batch(val_set_files, batch_size),\n",
    "            validation_steps=len(val_set_files) // batch_size, \n",
    "                    callbacks=[logging, checkpoint, early_stopping])\n",
    "model.save_weights('trained_stage_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Evaluate the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 1 and 50. Shapes are [1,1,1024,425] and [50,1024,1,1]. for 'Assign_558' (op: 'Assign') with input shapes: [1,1,1024,425], [50,1024,1,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 1 and 50. Shapes are [1,1,1024,425] and [50,1024,1,1]. for 'Assign_558' (op: 'Assign') with input shapes: [1,1,1024,425], [50,1024,1,1].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-8fa037c7bb19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the weights of the last trained model onto the loaded model(model_body: YOLO model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trained_stage_1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trained_stage_2_best.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# model_body.load_weights('trained_stage_2.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                 load_weights_from_hdf5_group(\n\u001b[0;32m-> 2667\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   3391\u001b[0m                              ' elements.')\n\u001b[1;32m   3392\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3393\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2370\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[1;32m   2371\u001b[0m                                                     shape=value.shape)\n\u001b[0;32m-> 2372\u001b[0;31m                 \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2373\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2374\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking)\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     \"\"\"\n\u001b[0;32m--> 615\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    281\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    282\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m\"Assign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1734\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1735\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 1 and 50. Shapes are [1,1,1024,425] and [50,1024,1,1]. for 'Assign_558' (op: 'Assign') with input shapes: [1,1,1024,425], [50,1024,1,1]."
     ]
    }
   ],
   "source": [
    "## Evaluating\n",
    "\n",
    "# Create the model again??: Do it if not loaded\n",
    "model_body, model = create_model(anchors, class_names, load_pretrained=False, freeze_body=False)\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "# Load the weights of the last trained model onto the loaded model(model_body: YOLO model)\n",
    "model_body.load_weights('trained_stage_1.h5')\n",
    "model_body.load_weights('trained_stage_2_best.h5')\n",
    "# model_body.load_weights('trained_stage_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_body.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before modifications, Original YOLO model had a head (Output tensors):\n",
    "\n",
    "* (<tf.Tensor 'truediv_2:0' shape=(?, ?, ?, 5, 2) dtype=float32>,\n",
    "* <tf.Tensor 'truediv_3:0' shape=(?, ?, ?, 5, 2) dtype=float32>,\n",
    "*  <tf.Tensor 'Sigmoid_1:0' shape=(?, ?, ?, 5, 1) dtype=float32>,\n",
    "* <tf.Tensor 'Reshape_7:0' shape=(?, ?, ?, 5, 80) dtype=float32>)\n",
    "\n",
    "Now, since we redisigned the output layer (2D convolution) for other classes, the output classes count would be diferrent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_outputs = yolo_head(model_body.output, anchors, len(class_names))\n",
    "yolo_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_shape = K.placeholder(shape=(2, ))\n",
    "input_image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_any(sess , model, image_file, anchors, class_names, max_boxes, score_threshold, iou_threshold):\n",
    "\n",
    "    # Get head of model\n",
    "    yolo_outputs_ = yolo_head(model.output, anchors, len(class_names))\n",
    "    \n",
    "    # Preprocess your image\n",
    "    model_image_size =  model.inputs[0].get_shape().as_list()[-3:-1]\n",
    "    image, image_data = preprocess_image(image_file, model_image_size =  model_image_size )\n",
    "    \n",
    "    img=plt.imread(image_file)\n",
    "    img_shape_ = img.shape[0:2]\n",
    "    print(  \"Reshaping input image \"+str( img_shape_)  +\" to model input shape \"+str(model_image_size)  )\n",
    "    \n",
    "    # Get the Tensors\n",
    "    boxes, scores, classes = yolo_eval(yolo_outputs_, [float(i) for i in list(img_shape_)],\n",
    "                max_boxes,\n",
    "              score_threshold,\n",
    "              iou_threshold)  \n",
    "\n",
    "    # Run the session with the correct tensors and choose the correct placeholders in the feed_dict.\n",
    "    # You'll need to use feed_dict={model.input: ... , K.learning_phase(): 0})\n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    out_boxes, out_scores, out_classes = sess.run(\n",
    "            [boxes, scores, classes],\n",
    "            feed_dict={\n",
    "                model.input: image_data,\n",
    "                input_image_shape: [image_data.shape[2], image_data.shape[3]],\n",
    "                K.learning_phase(): 0\n",
    "            })\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print predictions info\n",
    "    print('Found {} boxes for {}'.format(len(out_boxes), image_file))\n",
    "    # Generate colors for drawing bounding boxes.\n",
    "    colors = generate_colors(class_names)\n",
    "    # Draw bounding boxes on the image file\n",
    "    draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)\n",
    "    # Save the predicted bounding box on the image\n",
    "    image.save(os.path.join(\"out\", image_file.split('/')[-1]), quality=90)\n",
    "    # Display the results in the notebook\n",
    "    plt.figure()\n",
    "\n",
    "    output_image = scipy.misc.imread(os.path.join(\"out\", image_file.split('/')[-1]))\n",
    "    plt.imshow(output_image)\n",
    "\n",
    "    return out_scores, out_boxes, out_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts_files = os.listdir(images_dir+str(1))\n",
    "plt.figure()\n",
    "out_scores, out_boxes, out_classes = predict_any(sess, model_body,\n",
    "                                                 images_dir+str(1)+'/'+dts_files[ np.random.randint(0,high=len(dts_files)) ],\n",
    "                                                 anchors, class_names,\n",
    "                                                max_boxes=10,\n",
    "                                                  score_threshold=.6,\n",
    "                                                      iou_threshold = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OMdut",
   "launcher_item_id": "bbBOL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
